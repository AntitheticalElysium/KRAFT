{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f74ee9f6",
   "metadata": {},
   "source": [
    "# KRAFT: Model Evaluation\n",
    "\n",
    "This notebook evaluates the performance of our trained recommender system components. We will focus on two main types of evaluation:\n",
    "\n",
    "1.  **Evaluation on `small_matrix` (Dense Subset):**\n",
    "    *   Since `small_matrix` represents a nearly fully-observed set of interactions for a subset of users and items (that were also part of the `big_matrix` training data), we can perform a detailed evaluation here.\n",
    "    *   We will primarily evaluate the **LightGBM ranker** directly on all (user, item) pairs within the `small_matrix` scope.\n",
    "    *   Metrics will include pointwise prediction accuracy (RMSE, MAE for `watch_ratio`) and ranking metrics (Precision@k, Recall@k, nDCG@k).\n",
    "\n",
    "2.  **End-to-End Evaluation on `big_matrix` Holdout Set (Standard Evaluation):**\n",
    "    *   This involves the two-stage recommendation process:\n",
    "        1.  **Candidate Generation:** Use the trained ALS model to generate a list of candidate items for each user in the test set (derived from `big_matrix`).\n",
    "        2.  **Ranking:** Use the trained LightGBM model to re-rank these candidates.\n",
    "    *   Metrics will focus on ranking performance (Precision@k, Recall@k, nDCG@k) of the final re-ranked list against the ground truth interactions in the `big_matrix` test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca70a8f",
   "metadata": {},
   "source": [
    "## 1. Imports and Configuration\n",
    "\n",
    "Import necessary libraries, load custom evaluation metric functions from `utils.py`, and define paths and evaluation parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddae2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import implicit # For ALS model loading, though joblib is used for saving/loading\n",
    "import joblib\n",
    "import gc\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm # For progress bars\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Add scripts directory to sys.path to import utils\n",
    "import sys\n",
    "current_dir = os.getcwd()\n",
    "# Handle potential nested notebook structure for pathing\n",
    "if os.path.basename(current_dir) == 'notebooks':\n",
    "    base_dir = os.path.dirname(current_dir)\n",
    "else:\n",
    "    base_dir = current_dir\n",
    "scripts_path = os.path.join(base_dir, 'scripts')\n",
    "if scripts_path not in sys.path:\n",
    "    sys.path.append(scripts_path)\n",
    "\n",
    "from utils import precision_at_k, recall_at_k, ndcg_at_k\n",
    "\n",
    "# --- Path Definitions & Configuration ---\n",
    "PROCESSED_DATA_PATH = \"../data/\"\n",
    "MODELS_PATH = \"../models/\"\n",
    "TARGET_COL = 'watch_ratio'\n",
    "K_VALUES_FOR_RANKING = [5, 10, 20, 50, 100] # K values for ranking metrics\n",
    "RANKING_RELEVANCE_THRESHOLD = 1.0 # watch_ratio > threshold means relevant\n",
    "ALS_NUM_CANDIDATES = 250 # Number of candidates to generate per user from ALS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6b9a5c",
   "metadata": {},
   "source": [
    "## 2. Evaluation on `small_matrix` (Dense Subset)\n",
    "\n",
    "Here, we evaluate the LightGBM ranker's performance on the `small_matrix` data. Since this subset is fully observed for its specific users and items, we can directly assess pointwise prediction accuracy and ranking quality without a separate candidate generation step for this evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554b64fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n--- Evaluating LightGBM Ranker on Small Matrix ---\")\n",
    "\n",
    "# --- Load LightGBM Model ---\n",
    "print(\"Loading LightGBM model...\")\n",
    "lgbm_model_path = os.path.join(MODELS_PATH, \"lightgbm_ranker_model.txt\")\n",
    "try:\n",
    "    model_lgbm_loaded = lgb.Booster(model_file=lgbm_model_path)\n",
    "    print(\"LightGBM model loaded.\")\n",
    "except lgb.basic.LightGBMError as e:\n",
    "    print(f\"Error loading LightGBM model: {e}. Ensure the model file exists and is valid.\")\n",
    "    raise\n",
    "\n",
    "# --- Load Small Matrix Evaluation Data ---\n",
    "print(f\"\\nLoading small matrix evaluation data...\")\n",
    "small_matrix_eval_df_path = os.path.join(PROCESSED_DATA_PATH, 'small_matrix_eval_features_data.parquet')\n",
    "try:\n",
    "    df_small_eval = pd.read_parquet(small_matrix_eval_df_path)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {small_matrix_eval_df_path} not found. Please run Data Preparation notebook first.\")\n",
    "    raise\n",
    "print(f\"Loaded small matrix evaluation data: {df_small_eval.shape}\")\n",
    "\n",
    "# --- Prepare Features and Target for Small Matrix Evaluation ---\n",
    "# Identify feature columns (all columns except the target)\n",
    "lgbm_feature_columns_small_eval = [col for col in df_small_eval.columns if col != TARGET_COL]\n",
    "\n",
    "# Reconstruct the list of categorical feature names (must match training)\n",
    "onehot_feature_names_s_eval = [f'onehot_feat{i}' for i in range(18)]\n",
    "master_categorical_list_s_eval = ['user_id', 'video_id', 'user_active_degree', 'interaction_hour', 'interaction_day_of_week'] + \\\n",
    "                                 [flag for flag in ['is_lowactive_period', 'is_live_streamer', 'is_video_author'] if flag in df_small_eval.columns] + \\\n",
    "                                 onehot_feature_names_s_eval + \\\n",
    "                                 [feat for feat in ['author_id', 'video_type', 'video_tag_id'] if feat in df_small_eval.columns]\n",
    "\n",
    "print(\"\\nVerifying and casting categorical features for small matrix evaluation data...\")\n",
    "for col in master_categorical_list_s_eval:\n",
    "    if col in df_small_eval.columns:\n",
    "        if df_small_eval[col].dtype.name != 'category':\n",
    "            if df_small_eval[col].isnull().any():\n",
    "                if col in ['user_active_degree', 'video_type']:\n",
    "                    df_small_eval[col] = df_small_eval[col].astype(str).fillna(\"Unknown_Eval_NA\")\n",
    "                else:\n",
    "                    df_small_eval[col] = df_small_eval[col].fillna(-1)\n",
    "            df_small_eval[col] = df_small_eval[col].astype('category')\n",
    "\n",
    "X_small_eval_features = df_small_eval[lgbm_feature_columns_small_eval]\n",
    "actual_watch_ratios_small = df_small_eval[TARGET_COL]\n",
    "\n",
    "# --- Make Predictions ---\n",
    "print(\"\\nMaking predictions on small matrix data...\")\n",
    "predicted_watch_ratios_small = model_lgbm_loaded.predict(X_small_eval_features)\n",
    "df_small_eval['predicted_watch_ratio'] = predicted_watch_ratios_small\n",
    "print(\"Predictions complete for small matrix.\")\n",
    "gc.collect()\n",
    "\n",
    "# --- Pointwise Evaluation Metrics (RMSE, MAE) for Small Matrix ---\n",
    "print(f\"\\n--- Pointwise Evaluation on Small Matrix (Overall) ---\")\n",
    "rmse_small_overall = np.sqrt(mean_squared_error(actual_watch_ratios_small, predicted_watch_ratios_small))\n",
    "mae_small_overall = mean_absolute_error(actual_watch_ratios_small, predicted_watch_ratios_small)\n",
    "print(f\"Overall RMSE on Small Matrix: {rmse_small_overall:.4f}\")\n",
    "print(f\"Overall MAE on Small Matrix:  {mae_small_overall:.4f}\")\n",
    "\n",
    "# --- Ranking Evaluation Metrics for Small Matrix ---\n",
    "print(f\"\\n--- Ranking Evaluation on Small Matrix (Per User, then Averaged) ---\")\n",
    "print(f\"Using relevance threshold: watch_ratio > {RANKING_RELEVANCE_THRESHOLD}\")\n",
    "\n",
    "small_matrix_user_metrics = {k: {'precision': [], 'recall': [], 'ndcg': []} for k in K_VALUES_FOR_RANKING}\n",
    "unique_users_small_matrix = df_small_eval['user_id'].unique()\n",
    "num_eval_users_small = len(unique_users_small_matrix)\n",
    "print(f\"Evaluating ranking for {num_eval_users_small} users from small matrix...\")\n",
    "\n",
    "grouped_small_eval = df_small_eval.groupby('user_id', observed=True) # observed=True is good practice\n",
    "\n",
    "for user_id, user_data in tqdm(grouped_small_eval, desc=\"Small Matrix User Ranking Eval\"):\n",
    "    user_pred_items_sorted = user_data.sort_values(by='predicted_watch_ratio', ascending=False)['video_id'].tolist()\n",
    "    user_true_relevant_items = set(user_data[user_data[TARGET_COL] > RANKING_RELEVANCE_THRESHOLD]['video_id'].tolist())\n",
    "\n",
    "    if not user_true_relevant_items: continue\n",
    "\n",
    "    for k_val in K_VALUES_FOR_RANKING:\n",
    "        p_at_k = precision_at_k(user_true_relevant_items, user_pred_items_sorted, k_val)\n",
    "        r_at_k = recall_at_k(user_true_relevant_items, user_pred_items_sorted, k_val)\n",
    "        n_at_k = ndcg_at_k(user_true_relevant_items, user_pred_items_sorted, k_val)\n",
    "        \n",
    "        small_matrix_user_metrics[k_val]['precision'].append(p_at_k)\n",
    "        small_matrix_user_metrics[k_val]['recall'].append(r_at_k)\n",
    "        small_matrix_user_metrics[k_val]['ndcg'].append(n_at_k)\n",
    "\n",
    "print(\"\\nCalculating average ranking metrics for Small Matrix...\")\n",
    "avg_metrics_report_small = {}\n",
    "for k_val in K_VALUES_FOR_RANKING:\n",
    "    avg_precision = np.mean(small_matrix_user_metrics[k_val]['precision']) if small_matrix_user_metrics[k_val]['precision'] else 0\n",
    "    avg_recall = np.mean(small_matrix_user_metrics[k_val]['recall']) if small_matrix_user_metrics[k_val]['recall'] else 0\n",
    "    avg_ndcg = np.mean(small_matrix_user_metrics[k_val]['ndcg']) if small_matrix_user_metrics[k_val]['ndcg'] else 0\n",
    "    \n",
    "    avg_metrics_report_small[f\"Precision@{k_val}\"] = avg_precision\n",
    "    avg_metrics_report_small[f\"Recall@{k_val}\"] = avg_recall\n",
    "    avg_metrics_report_small[f\"nDCG@{k_val}\"] = avg_ndcg\n",
    "    \n",
    "    print(f\"  Avg Precision@{k_val} (Small Matrix): {avg_precision:.4f}\")\n",
    "    print(f\"  Avg Recall@{k_val}    (Small Matrix): {avg_recall:.4f}\")\n",
    "    print(f\"  Avg nDCG@{k_val}      (Small Matrix): {avg_ndcg:.4f}\")\n",
    "\n",
    "del df_small_eval, X_small_eval_features, actual_watch_ratios_small, predicted_watch_ratios_small, grouped_small_eval\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d9618d",
   "metadata": {},
   "source": [
    "## 3. End-to-End Evaluation on `big_matrix` Holdout Set\n",
    "\n",
    "This is the standard evaluation for the two-stage recommender system. It involves:\n",
    "1. Loading the ALS model, LightGBM model, ID mappings, and the `big_matrix` test data (`lightgbm_test_data.parquet` and ground truth).\n",
    "2. For each user in the test set:\n",
    "    a. Use ALS to generate top-N candidate `video_id`s.\n",
    "    b. Filter out candidates the user has already interacted with in the training set (if applicable, though our test set is time-split which mitigates this for *future* interactions).\n",
    "    c. Construct feature vectors for these (user, candidate_item) pairs.\n",
    "    d. Use LightGBM to predict `watch_ratio` for these candidates and re-rank them.\n",
    "3. Calculate ranking metrics (P@k, R@k, nDCG@k) by comparing the re-ranked list against the user's actual interactions in the `big_matrix` test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9ace12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n--- End-to-End Evaluation on Big Matrix Holdout Set ---\")\n",
    "\n",
    "# --- Load ALS Model and ID Mappings ---\n",
    "print(\"Loading ALS model and ID mappings...\")\n",
    "als_model_path = os.path.join(MODELS_PATH, \"als_model.joblib\")\n",
    "try:\n",
    "    als_model_loaded = joblib.load(als_model_path)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: ALS model not found at {als_model_path}. Please run Model Training notebook.\")\n",
    "    raise\n",
    "\n",
    "id_mapping_files = ['user_to_idx_als.json', 'idx_to_user_als.json', 'video_to_idx_als.json', 'idx_to_video_als.json']\n",
    "id_mappings = {}\n",
    "try:\n",
    "    for f_name in id_mapping_files:\n",
    "        with open(os.path.join(PROCESSED_DATA_PATH, f_name), 'r') as f:\n",
    "            # JSON keys are always strings, convert to int if they represent integer IDs\n",
    "            mapping = json.load(f)\n",
    "            if 'idx_to' in f_name: # Keys are stringified integers from 0...N-1\n",
    "                id_mappings[f_name.split('.')[0]] = {int(k): v for k,v in mapping.items()}\n",
    "            else: # user_to_idx, video_to_idx: keys are original IDs, should be int\n",
    "                id_mappings[f_name.split('.')[0]] = {int(k): v for k,v in mapping.items()}\n",
    "    print(\"ALS ID mappings loaded.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: ALS ID mapping files not found. Please run Data Preparation notebook.\")\n",
    "    raise\n",
    "\n",
    "user_to_idx_als = id_mappings['user_to_idx_als']\n",
    "idx_to_user_als = id_mappings['idx_to_user_als']\n",
    "video_to_idx_als = id_mappings['video_to_idx_als']\n",
    "idx_to_video_als = id_mappings['idx_to_video_als']\n",
    "\n",
    "# --- Load Big Matrix Test Data and Ground Truth ---\n",
    "print(\"\\nLoading LightGBM test data (big matrix holdout)...\")\n",
    "test_lgbm_parquet_path = os.path.join(PROCESSED_DATA_PATH, 'lightgbm_test_data.parquet')\n",
    "try:\n",
    "    df_test_lgbm = pd.read_parquet(test_lgbm_parquet_path) # Features + target\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {test_lgbm_parquet_path} not found. Please run Data Preparation.\")\n",
    "    raise\n",
    "\n",
    "print(\"Loading ground truth for big matrix test set...\")\n",
    "ground_truth_big_path = os.path.join(PROCESSED_DATA_PATH, 'ground_truth_test_big_matrix.csv')\n",
    "try:\n",
    "    df_ground_truth_big = pd.read_csv(ground_truth_big_path)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {ground_truth_big_path} not found. Please run Data Preparation.\")\n",
    "    raise\n",
    "\n",
    "print(f\"Loaded LGBM test data: {df_test_lgbm.shape}\")\n",
    "print(f\"Loaded ground truth data: {df_ground_truth_big.shape}\")\n",
    "\n",
    "# --- Prepare for Evaluation Loop ---\n",
    "# The LightGBM model (`model_lgbm_loaded`) is already loaded from the small matrix eval part.\n",
    "\n",
    "# Create a lookup for ground truth items per user from df_ground_truth_big\n",
    "user_true_items_big_test = df_ground_truth_big[\n",
    "    df_ground_truth_big[TARGET_COL] > RANKING_RELEVANCE_THRESHOLD\n",
    "].groupby('user_id')['video_id'].apply(set).to_dict()\n",
    "\n",
    "# The df_test_lgbm contains all features needed for LightGBM predictions for user-item pairs in the test set.\n",
    "# We need to simulate the candidate generation -> ranking pipeline.\n",
    "\n",
    "# Features that LightGBM model expects (excluding target)\n",
    "lgbm_feature_columns_big_test = [col for col in df_test_lgbm.columns if col != TARGET_COL]\n",
    "\n",
    "# Ensure categoricals are typed (should be from parquet, but good check)\n",
    "print(\"\\nVerifying and casting categorical features for big matrix test data...\")\n",
    "master_categorical_list_big_eval = ['user_id', 'video_id', 'user_active_degree', 'interaction_hour', 'interaction_day_of_week'] + \\\n",
    "                                   [flag for flag in ['is_lowactive_period', 'is_live_streamer', 'is_video_author'] if flag in df_test_lgbm.columns] + \\\n",
    "                                   [f'onehot_feat{i}' for i in range(18)] + \\\n",
    "                                   [feat for feat in ['author_id', 'video_type', 'video_tag_id'] if feat in df_test_lgbm.columns]\n",
    "\n",
    "for col in master_categorical_list_big_eval:\n",
    "    if col in df_test_lgbm.columns and df_test_lgbm[col].dtype.name != 'category':\n",
    "        if df_test_lgbm[col].isnull().any():\n",
    "            if col in ['user_active_degree', 'video_type']:\n",
    "                df_test_lgbm[col] = df_test_lgbm[col].astype(str).fillna(\"Unknown_Eval_NA\")\n",
    "            else:\n",
    "                df_test_lgbm[col] = df_test_lgbm[col].fillna(-1)\n",
    "        df_test_lgbm[col] = df_test_lgbm[col].astype('category')\n",
    "\n",
    "X_test_lgbm_features_all = df_test_lgbm[lgbm_feature_columns_big_test]\n",
    "\n",
    "# We need all unique items for constructing feature rows for candidates if they are not in X_test_lgbm_features_all\n",
    "# This requires access to item features. The simplest way is to use df_test_lgbm as a source of features for known interactions,\n",
    "# and for ALS candidates not in df_test_lgbm, we'd ideally build their features from scratch.\n",
    "# For this evaluation, we'll rank items ALREADY IN THE TEST SET for simplicity of feature access.\n",
    "# A more complete pipeline would fetch/build features for any arbitrary candidate item.\n",
    "\n",
    "print(f\"Evaluating end-to-end ranking for users in big matrix test set...\")\n",
    "big_matrix_e2e_metrics = {k: {'precision': [], 'recall': [], 'ndcg': []} for k in K_VALUES_FOR_RANKING}\n",
    "test_users_big_matrix = df_test_lgbm['user_id'].unique()\n",
    "\n",
    "# For constructing features for (user, candidate_item) pairs, we need user features and item features.\n",
    "# df_test_lgbm already has interaction-level merged features.\n",
    "# If an ALS candidate is NOT in a user's test interactions, we need to create its feature row.\n",
    "# This is complex. Let's simplify: rank only items available in the test set for that user.\n",
    "# This means ALS isn't truly used for candidate *generation* here, but for an *initial ordering/signal* if we wanted.\n",
    "# For now, let's evaluate LGBM ranking on items present in test set, similar to small_matrix eval.\n",
    "\n",
    "print(\"Evaluating LightGBM ranking on items present in the Big Matrix test set (similar to small matrix eval)...\")\n",
    "if 'predicted_watch_ratio' not in df_test_lgbm.columns:\n",
    "    df_test_lgbm['predicted_watch_ratio'] = model_lgbm_loaded.predict(X_test_lgbm_features_all)\n",
    "    print(\"Predictions made for big matrix test data.\")\n",
    "gc.collect()\n",
    "\n",
    "grouped_test_lgbm = df_test_lgbm.groupby('user_id', observed=True)\n",
    "for user_id, user_data in tqdm(grouped_test_lgbm, desc=\"Big Matrix Test User Ranking Eval\"):\n",
    "    user_pred_items_sorted = user_data.sort_values(by='predicted_watch_ratio', ascending=False)['video_id'].tolist()\n",
    "    true_relevant_for_user = user_true_items_big_test.get(user_id, set()) # Get from precomputed ground truth\n",
    "\n",
    "    if not true_relevant_for_user: continue\n",
    "\n",
    "    for k_val in K_VALUES_FOR_RANKING:\n",
    "        p_at_k = precision_at_k(true_relevant_for_user, user_pred_items_sorted, k_val)\n",
    "        r_at_k = recall_at_k(true_relevant_for_user, user_pred_items_sorted, k_val)\n",
    "        n_at_k = ndcg_at_k(true_relevant_for_user, user_pred_items_sorted, k_val)\n",
    "        \n",
    "        big_matrix_e2e_metrics[k_val]['precision'].append(p_at_k)\n",
    "        big_matrix_e2e_metrics[k_val]['recall'].append(r_at_k)\n",
    "        big_matrix_e2e_metrics[k_val]['ndcg'].append(n_at_k)\n",
    "\n",
    "print(\"\\nCalculating average ranking metrics for Big Matrix Test Set (LGBM ranking on test items)...\")\n",
    "avg_metrics_report_big_lgbm_only = {}\n",
    "for k_val in K_VALUES_FOR_RANKING:\n",
    "    avg_precision = np.mean(big_matrix_e2e_metrics[k_val]['precision']) if big_matrix_e2e_metrics[k_val]['precision'] else 0\n",
    "    avg_recall = np.mean(big_matrix_e2e_metrics[k_val]['recall']) if big_matrix_e2e_metrics[k_val]['recall'] else 0\n",
    "    avg_ndcg = np.mean(big_matrix_e2e_metrics[k_val]['ndcg']) if big_matrix_e2e_metrics[k_val]['ndcg'] else 0\n",
    "    \n",
    "    avg_metrics_report_big_lgbm_only[f\"Precision@{k_val}\"] = avg_precision\n",
    "    avg_metrics_report_big_lgbm_only[f\"Recall@{k_val}\"] = avg_recall\n",
    "    avg_metrics_report_big_lgbm_only[f\"nDCG@{k_val}\"] = avg_ndcg\n",
    "    \n",
    "    print(f\"  Avg Precision@{k_val} (Big Matrix Test - LGBM on test items): {avg_precision:.4f}\")\n",
    "    print(f\"  Avg Recall@{k_val}    (Big Matrix Test - LGBM on test items): {avg_recall:.4f}\")\n",
    "    print(f\"  Avg nDCG@{k_val}      (Big Matrix Test - LGBM on test items): {avg_ndcg:.4f}\")\n",
    "\n",
    "# --- Store all metrics --- \n",
    "final_evaluation_report = {\n",
    "    \"small_matrix_pointwise\": {\n",
    "        \"rmse\": rmse_small_overall,\n",
    "        \"mae\": mae_small_overall\n",
    "    },\n",
    "    \"small_matrix_ranking\": avg_metrics_report_small,\n",
    "    \"big_matrix_test_lgbm_rank_on_test_items\": avg_metrics_report_big_lgbm_only\n",
    "    # Add true end-to-end (ALS+LGBM) metrics here once implemented\n",
    "}\n",
    "\n",
    "metrics_output_path = os.path.join(PROCESSED_DATA_PATH, \"full_evaluation_metrics.json\")\n",
    "with open(metrics_output_path, 'w') as f:\n",
    "    json.dump(final_evaluation_report, f, indent=4)\n",
    "print(f\"\\nFull evaluation metrics saved to: {metrics_output_path}\")\n",
    "\n",
    "print(\"\\n--- Model Evaluation Phase Complete ---\")\n",
    "\n",
    "# Placeholder for True End-to-End ALS + LGBM evaluation on big_matrix test set\n",
    "print(\"\\n--- Placeholder: True End-to-End ALS + LGBM Evaluation on Big Matrix Test ---\")\n",
    "print(\"This section requires generating candidates with ALS for each test user, \")\n",
    "print(\"then building features for (user, candidate_item) pairs and ranking with LightGBM.\")\n",
    "print(\"This is more involved due to feature construction for arbitrary candidates and will be implemented separately if needed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (implicit-env)",
   "language": "python",
   "name": "implicit-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
