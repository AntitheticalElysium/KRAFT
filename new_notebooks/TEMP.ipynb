{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33b551bf-76c3-4787-bbda-defc09a69964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Phase 1: Data Preparation & Feature Engineering (Slightly Richer V3) ---\n",
      "--- Step 1.1: Loading Selected Raw Data & Initial Cleaning ---\n",
      "Loading big_matrix.csv...\n",
      "Loaded and processed big_matrix: (12530806, 8)\n",
      "\n",
      "Loading small_matrix.csv...\n",
      "Loaded and processed small_matrix: (4676570, 8)\n",
      "\n",
      "Loading user_features.csv (selected columns)...\n",
      "Loaded user_features: (7176, 26)\n",
      "\n",
      "Loading item_categories.csv...\n",
      "Loaded item_categories: (10728, 2)\n",
      "\n",
      "Loading item_daily_features.csv (initial load for processing)...\n",
      "Processed initial item_daily_features: (343341, 12)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from scipy.sparse import csr_matrix\n",
    "import json\n",
    "import os\n",
    "\n",
    "RAW_DATA_BASE_PATH = \"../raw_data/KuaiRec/data/\"\n",
    "PROCESSED_DATA_PATH = \"../data/\"\n",
    "MODELS_PATH = \"../models/\"\n",
    "os.makedirs(PROCESSED_DATA_PATH, exist_ok=True)\n",
    "os.makedirs(MODELS_PATH, exist_ok=True)\n",
    "\n",
    "# --- Dtype Definitions & Column Selections\n",
    "interaction_cols_initial_load = {\n",
    "    'user_id': 'float32', 'video_id': 'float32', 'play_duration': 'float32',\n",
    "    'video_duration': 'float32', 'time': 'str', 'date': 'float32',\n",
    "    'timestamp': 'float32', 'watch_ratio': 'float32'\n",
    "}\n",
    "interaction_cols_final_dtypes = {\n",
    "    'user_id': 'int32', 'video_id': 'int32', 'play_duration': 'int32',\n",
    "    'video_duration': 'int32', 'time': 'str', 'date': 'int32',\n",
    "    'timestamp': 'float32', 'watch_ratio': 'float32'\n",
    "}\n",
    "\n",
    "# User features\n",
    "user_features_selected_cols = {\n",
    "    'user_id': 'int32', 'user_active_degree': 'category',\n",
    "    'is_lowactive_period': 'float32', # ADDED BACK\n",
    "    'is_live_streamer': 'float32', 'is_video_author': 'float32',\n",
    "    'follow_user_num': 'int32',      # ADDED BACK\n",
    "    'fans_user_num': 'int32', 'register_days': 'int32',\n",
    "}\n",
    "onehot_feature_names = []\n",
    "for i in range(18):\n",
    "    col_name = f'onehot_feat{i}'\n",
    "    user_features_selected_cols[col_name] = 'float32'\n",
    "    onehot_feature_names.append(col_name)\n",
    "\n",
    "item_categories_selected_cols = {'video_id': 'int32', 'feat': 'str'}\n",
    "\n",
    "# Item daily features\n",
    "item_daily_initial_load_cols = {\n",
    "    'video_id': 'float32', 'date': 'float32', 'author_id': 'float32',\n",
    "    'video_type': 'category', 'upload_dt': 'str', 'video_duration': 'float32',\n",
    "    'show_cnt': 'float32', 'play_cnt': 'float32', 'like_cnt': 'float32',\n",
    "    'complete_play_cnt': 'float32', 'play_progress': 'float32',\n",
    "    'video_tag_id': 'float32',\n",
    "}\n",
    "item_daily_cols_to_keep_for_merge = [\n",
    "    'video_id', 'item_stats_date', 'author_id', 'video_type', 'upload_dt_parsed',\n",
    "    'video_duration_daily', 'daily_play_progress',\n",
    "    'daily_play_per_show_ratio', 'daily_like_per_play_ratio', 'daily_completion_rate',\n",
    "    'video_tag_id'\n",
    "]\n",
    "\n",
    "print(\"--- Phase 1: Data Preparation & Feature Engineering (Slightly Richer V3) ---\")\n",
    "print(\"--- Step 1.1: Loading Selected Raw Data & Initial Cleaning ---\")\n",
    "\n",
    "def post_process_interaction_df(df, final_dtypes_map):\n",
    "    int_cols_with_potential_na = ['user_id', 'video_id', 'play_duration', 'video_duration', 'date']\n",
    "    for col in int_cols_with_potential_na:\n",
    "        if col in df.columns:\n",
    "            fill_value = -1 if ('id' in col or 'date' in col) else 0\n",
    "            df[col] = df[col].fillna(fill_value).astype(final_dtypes_map[col])\n",
    "    for col in ['timestamp', 'watch_ratio']:\n",
    "        if col in df.columns: df[col] = df[col].astype(final_dtypes_map[col])\n",
    "    return df\n",
    "\n",
    "print(\"Loading big_matrix.csv...\")\n",
    "df_big_interactions = pd.read_csv(os.path.join(RAW_DATA_BASE_PATH, \"big_matrix.csv\"),\n",
    "                                  usecols=interaction_cols_initial_load.keys(),\n",
    "                                  dtype=interaction_cols_initial_load)\n",
    "df_big_interactions = post_process_interaction_df(df_big_interactions, interaction_cols_final_dtypes)\n",
    "print(f\"Loaded and processed big_matrix: {df_big_interactions.shape}\")\n",
    "\n",
    "print(\"\\nLoading small_matrix.csv...\")\n",
    "df_small_interactions = pd.read_csv(os.path.join(RAW_DATA_BASE_PATH, \"small_matrix.csv\"),\n",
    "                                    usecols=interaction_cols_initial_load.keys(),\n",
    "                                    dtype=interaction_cols_initial_load)\n",
    "df_small_interactions = post_process_interaction_df(df_small_interactions, interaction_cols_final_dtypes)\n",
    "print(f\"Loaded and processed small_matrix: {df_small_interactions.shape}\")\n",
    "\n",
    "print(\"\\nLoading user_features.csv (selected columns)...\")\n",
    "df_user_features = pd.read_csv(os.path.join(RAW_DATA_BASE_PATH, \"user_features.csv\"),\n",
    "                               usecols=user_features_selected_cols.keys(),\n",
    "                               dtype=user_features_selected_cols)\n",
    "for col in onehot_feature_names:\n",
    "    df_user_features[col] = df_user_features[col].fillna(-1).astype('int16')\n",
    "for col in ['is_lowactive_period', 'is_live_streamer', 'is_video_author']: # ADDED is_lowactive_period\n",
    "    if col in df_user_features.columns: # Check if loaded\n",
    "        if df_user_features[col].isnull().any(): df_user_features[col] = df_user_features[col].fillna(-1).astype('int8')\n",
    "        else: df_user_features[col] = df_user_features[col].astype('int8')\n",
    "print(f\"Loaded user_features: {df_user_features.shape}\")\n",
    "\n",
    "print(\"\\nLoading item_categories.csv...\")\n",
    "df_item_categories = pd.read_csv(os.path.join(RAW_DATA_BASE_PATH, \"item_categories.csv\"),\n",
    "                                 usecols=item_categories_selected_cols.keys(),\n",
    "                                 dtype=item_categories_selected_cols)\n",
    "def parse_feat_list(feat_str):\n",
    "    if isinstance(feat_str, list): return len(feat_str)\n",
    "    if pd.isna(feat_str) or not isinstance(feat_str, str): return 0\n",
    "    try: return len(eval(feat_str))\n",
    "    except: return 0\n",
    "df_item_categories['num_item_tags'] = df_item_categories['feat'].apply(parse_feat_list).astype('int16')\n",
    "df_item_categories = df_item_categories.drop(columns=['feat'])\n",
    "print(f\"Loaded item_categories: {df_item_categories.shape}\")\n",
    "\n",
    "print(\"\\nLoading item_daily_features.csv (initial load for processing)...\")\n",
    "df_item_daily_full_load = pd.read_csv(os.path.join(RAW_DATA_BASE_PATH, \"item_daily_features.csv\"),\n",
    "                                usecols=item_daily_initial_load_cols.keys(),\n",
    "                                dtype=item_daily_initial_load_cols)\n",
    "df_item_daily_processed = df_item_daily_full_load.rename(columns={\n",
    "    'date': 'item_stats_date', 'video_duration': 'video_duration_daily',\n",
    "    'show_cnt': 'daily_show_cnt', 'play_cnt': 'daily_play_cnt',\n",
    "    'like_cnt': 'daily_like_cnt', 'complete_play_cnt': 'daily_complete_play_cnt',\n",
    "    'play_progress': 'daily_play_progress'\n",
    "})\n",
    "del df_item_daily_full_load\n",
    "gc.collect()\n",
    "\n",
    "# NA fill and type conversion for item_daily_processed\n",
    "id_cols_daily = ['video_id', 'author_id', 'video_tag_id'] # Added video_tag_id\n",
    "count_cols_daily = ['daily_show_cnt', 'daily_play_cnt', 'daily_like_cnt', 'daily_complete_play_cnt']\n",
    "for col in id_cols_daily:\n",
    "    if col in df_item_daily_processed.columns:\n",
    "        df_item_daily_processed[col] = df_item_daily_processed[col].fillna(-1).astype('int32')\n",
    "if 'item_stats_date' in df_item_daily_processed.columns: # item_stats_date was original 'date'\n",
    "    df_item_daily_processed['item_stats_date'] = df_item_daily_processed['item_stats_date'].fillna(-1).astype('int32')\n",
    "for col in count_cols_daily:\n",
    "    if col in df_item_daily_processed.columns:\n",
    "        df_item_daily_processed[col] = df_item_daily_processed[col].fillna(0).astype('int32')\n",
    "if 'video_duration_daily' in df_item_daily_processed.columns:\n",
    "    df_item_daily_processed['video_duration_daily'] = df_item_daily_processed['video_duration_daily'].fillna(0).astype('float32')\n",
    "if 'daily_play_progress' in df_item_daily_processed.columns:\n",
    "    df_item_daily_processed['daily_play_progress'] = df_item_daily_processed['daily_play_progress'].fillna(0).astype('float32')\n",
    "print(f\"Processed initial item_daily_features: {df_item_daily_processed.shape}\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f84dc9ca-fef3-42b4-a290-8c8f1f552c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Globally processing df_item_daily for derived features & slimming...\n",
      "Slimmed df_item_daily_lean_for_merge for merging. Shape: (343341, 11), Columns: ['video_id', 'item_stats_date', 'author_id', 'video_type', 'upload_dt_parsed', 'video_duration_daily', 'daily_play_progress', 'daily_play_per_show_ratio', 'daily_like_per_play_ratio', 'daily_completion_rate', 'video_tag_id']\n",
      "\n",
      "Checking for duplicate keys in df_item_daily_lean_for_merge...\n",
      "Number of rows involved in duplicate (video_id, item_stats_date) keys: 226111\n",
      "Shape of df_item_daily_lean_for_merge after dropping duplicates: (194155, 11)\n",
      "\n",
      "--- Step 1.3: Apply Feature Engineering to Big Matrix Interactions ---\n",
      "Engineering features for interaction table with shape: (12530806, 8)\n",
      "Merging user features...\n",
      "Shape after user features merge: (12530806, 36)\n",
      "Merging static item features...\n",
      "Shape after static item features merge: (12530806, 37)\n",
      "Merging dynamic item features (lean)...\n",
      "Shape after dynamic item features merge: (12530806, 47)\n",
      "Post-merge feature engineering...\n",
      "Finished engineering. Shape: (12530806, 48)\n",
      "\n",
      "--- Step 1.4: Apply Feature Engineering to Small Matrix Interactions ---\n",
      "Engineering features for interaction table with shape: (4676570, 8)\n",
      "Merging user features...\n",
      "Shape after user features merge: (4676570, 36)\n",
      "Merging static item features...\n",
      "Shape after static item features merge: (4676570, 37)\n",
      "Merging dynamic item features (lean)...\n",
      "Shape after dynamic item features merge: (4676570, 47)\n",
      "Post-merge feature engineering...\n",
      "Finished engineering. Shape: (4676570, 48)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Step 1.2: Globally process df_item_daily for derived features & make it ultra-lean ---\n",
    "print(\"\\nGlobally processing df_item_daily for derived features & slimming...\")\n",
    "if 'upload_dt' in df_item_daily_processed.columns:\n",
    "    df_item_daily_processed['upload_dt_parsed'] = pd.to_datetime(df_item_daily_processed['upload_dt'], errors='coerce')\n",
    "else:\n",
    "    df_item_daily_processed['upload_dt_parsed'] = pd.NaT # Placeholder if not loaded\n",
    "\n",
    "# Ratio calculations (with checks)\n",
    "if 'daily_play_cnt' in df_item_daily_processed.columns and 'daily_show_cnt' in df_item_daily_processed.columns:\n",
    "    df_item_daily_processed['daily_play_per_show_ratio'] = (df_item_daily_processed['daily_play_cnt'] / (df_item_daily_processed['daily_show_cnt'] + 1e-6)).astype('float32')\n",
    "else:\n",
    "    df_item_daily_processed['daily_play_per_show_ratio'] = np.float32(0.0)\n",
    "if 'daily_like_cnt' in df_item_daily_processed.columns and 'daily_play_cnt' in df_item_daily_processed.columns:\n",
    "    df_item_daily_processed['daily_like_per_play_ratio'] = (df_item_daily_processed['daily_like_cnt'] / (df_item_daily_processed['daily_play_cnt'] + 1e-6)).astype('float32')\n",
    "else:\n",
    "    df_item_daily_processed['daily_like_per_play_ratio'] = np.float32(0.0)\n",
    "if 'daily_complete_play_cnt' in df_item_daily_processed.columns and 'daily_play_cnt' in df_item_daily_processed.columns:\n",
    "    df_item_daily_processed['daily_completion_rate'] = (df_item_daily_processed['daily_complete_play_cnt'] / (df_item_daily_processed['daily_play_cnt'] + 1e-6)).astype('float32')\n",
    "else:\n",
    "    df_item_daily_processed['daily_completion_rate'] = np.float32(0.0)\n",
    "\n",
    "if 'video_type' in df_item_daily_processed.columns and df_item_daily_processed['video_type'].dtype.name != 'category':\n",
    "    df_item_daily_processed['video_type'] = df_item_daily_processed['video_type'].astype('category')\n",
    "    if 'Unknown' not in df_item_daily_processed['video_type'].cat.categories:\n",
    "        df_item_daily_processed['video_type'] = df_item_daily_processed['video_type'].cat.add_categories('Unknown')\n",
    "\n",
    "actual_cols_to_keep = [col for col in item_daily_cols_to_keep_for_merge if col in df_item_daily_processed.columns]\n",
    "df_item_daily_lean_for_merge = df_item_daily_processed[actual_cols_to_keep].copy()\n",
    "print(f\"Slimmed df_item_daily_lean_for_merge for merging. Shape: {df_item_daily_lean_for_merge.shape}, Columns: {df_item_daily_lean_for_merge.columns.tolist()}\")\n",
    "\n",
    "print(\"\\nChecking for duplicate keys in df_item_daily_lean_for_merge...\")\n",
    "key_cols_daily = ['video_id', 'item_stats_date']\n",
    "actual_key_cols_daily = [col for col in key_cols_daily if col in df_item_daily_lean_for_merge.columns]\n",
    "if len(actual_key_cols_daily) == len(key_cols_daily):\n",
    "    duplicate_daily_keys = df_item_daily_lean_for_merge.duplicated(subset=actual_key_cols_daily, keep=False)\n",
    "    num_duplicate_daily_keys = duplicate_daily_keys.sum()\n",
    "    print(f\"Number of rows involved in duplicate ({', '.join(actual_key_cols_daily)}) keys: {num_duplicate_daily_keys}\")\n",
    "    if num_duplicate_daily_keys > 0:\n",
    "        df_item_daily_lean_for_merge = df_item_daily_lean_for_merge.drop_duplicates(subset=actual_key_cols_daily, keep='first').copy()\n",
    "        print(f\"Shape of df_item_daily_lean_for_merge after dropping duplicates: {df_item_daily_lean_for_merge.shape}\")\n",
    "else:\n",
    "    print(f\"Warning: Key columns for daily features duplicate check not found. Skipping.\")\n",
    "del df_item_daily_processed\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "def engineer_features_for_interactions(df_interactions_raw, df_user_features_ref, df_item_categories_ref, df_item_daily_ref_lean):\n",
    "    print(f\"Engineering features for interaction table with shape: {df_interactions_raw.shape}\")\n",
    "    df = df_interactions_raw.copy()\n",
    "\n",
    "    df['interaction_datetime'] = pd.to_datetime(df['time'], errors='coerce')\n",
    "    df['interaction_hour'] = df['interaction_datetime'].dt.hour.fillna(-1).astype('int8')\n",
    "    df['interaction_day_of_week'] = df['interaction_datetime'].dt.dayofweek.fillna(-1).astype('int8')\n",
    "    df = df.rename(columns={'date': 'interaction_date', 'video_duration': 'video_duration_interaction'})\n",
    "\n",
    "    print(\"Merging user features...\")\n",
    "    df = pd.merge(df, df_user_features_ref, on='user_id', how='left')\n",
    "    print(f\"Shape after user features merge: {df.shape}\")\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"Merging static item features...\")\n",
    "    df = pd.merge(df, df_item_categories_ref, on='video_id', how='left')\n",
    "    df['num_item_tags'] = df['num_item_tags'].fillna(0).astype('int16')\n",
    "    print(f\"Shape after static item features merge: {df.shape}\")\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"Merging dynamic item features (lean)...\")\n",
    "    df = pd.merge(df, df_item_daily_ref_lean,\n",
    "                     left_on=['video_id', 'interaction_date'],\n",
    "                     right_on=['video_id', 'item_stats_date'],\n",
    "                     how='left')\n",
    "    print(f\"Shape after dynamic item features merge: {df.shape}\")\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"Post-merge feature engineering...\")\n",
    "    if 'upload_dt_parsed' in df.columns and 'interaction_datetime' in df.columns:\n",
    "        video_age_delta = df['interaction_datetime'] - df['upload_dt_parsed']\n",
    "        df['video_age_days'] = video_age_delta.dt.days.fillna(-1).astype('int16')\n",
    "    else:\n",
    "        df['video_age_days'] = pd.Series([-1]*len(df), index=df.index, dtype='int16')\n",
    "\n",
    "    # Updated list based on item_daily_cols_to_keep_for_merge\n",
    "    cols_from_daily_merge_to_fill = [\n",
    "        'author_id', 'video_type', 'video_duration_daily', 'daily_play_progress',\n",
    "        'daily_play_per_show_ratio', 'daily_like_per_play_ratio', 'daily_completion_rate',\n",
    "        'video_tag_id' # ADDED for NA handling\n",
    "    ]\n",
    "    for col in cols_from_daily_merge_to_fill:\n",
    "        if col in df.columns:\n",
    "            if col in ['author_id', 'video_tag_id']: # video_tag_id is also an ID\n",
    "                df[col] = df[col].fillna(-1).astype('int32')\n",
    "            elif col == 'video_type':\n",
    "                if df[col].isnull().any():\n",
    "                    if df[col].dtype.name != 'category':\n",
    "                         df[col] = pd.Categorical(df[col]) \n",
    "                    current_categories = df[col].cat.categories.tolist()\n",
    "                    if 'Unknown' not in current_categories:\n",
    "                        df[col] = df[col].cat.add_categories('Unknown')\n",
    "                    df[col] = df[col].fillna('Unknown')\n",
    "            else: \n",
    "                df[col] = df[col].fillna(0).astype('float32')\n",
    "        else:\n",
    "            print(f\"Warning: Column '{col}' expected from daily merge not found post-merge.\")\n",
    "            \n",
    "    df = df.sort_values(by='timestamp').reset_index(drop=True)\n",
    "    print(f\"Finished engineering. Shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "# --- Apply Feature Engineering ---\n",
    "print(\"\\n--- Step 1.3: Apply Feature Engineering to Big Matrix Interactions ---\")\n",
    "df_big_merged = engineer_features_for_interactions(df_big_interactions, df_user_features, df_item_categories, df_item_daily_lean_for_merge)\n",
    "del df_big_interactions\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\n--- Step 1.4: Apply Feature Engineering to Small Matrix Interactions ---\")\n",
    "df_small_eval_features = engineer_features_for_interactions(df_small_interactions, df_user_features, df_item_categories, df_item_daily_lean_for_merge)\n",
    "del df_small_interactions\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35842de1-4166-482a-9115-80f934b7ef43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 1.5: Prepare Data for ALS (Candidate Generation from Big Matrix) ---\n",
      "ALS Sparse Matrix Shape: (7176, 10728)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Step 1.5: Prepare Data for ALS (from df_big_merged) ---\n",
    "print(\"\\n--- Step 1.5: Prepare Data for ALS (Candidate Generation from Big Matrix) ---\")\n",
    "# Check if 'user_id' and 'video_id' columns exist before proceeding\n",
    "if 'user_id' not in df_big_merged.columns or 'video_id' not in df_big_merged.columns:\n",
    "    raise ValueError(\"user_id or video_id not found in df_big_merged for ALS prep.\")\n",
    "\n",
    "unique_users_als_np = df_big_merged['user_id'].unique()\n",
    "unique_videos_als_np = df_big_merged['video_id'].unique()\n",
    "\n",
    "user_to_idx = {int(user_id): i for i, user_id in enumerate(unique_users_als_np)}\n",
    "idx_to_user = {i: int(user_id) for user_id, val in user_to_idx.items() for i, user_id_original_type in enumerate(unique_users_als_np) if int(user_id_original_type) == val } # More robust creation\n",
    "idx_to_user = {i: int(user_id) for i, user_id in enumerate(unique_users_als_np)} # Simpler assuming unique_users_als_np is in order of user_to_idx values.\n",
    "num_users_als = len(unique_users_als_np)\n",
    "\n",
    "video_to_idx = {int(video_id): i for i, video_id in enumerate(unique_videos_als_np)}\n",
    "idx_to_video = {i: int(video_id) for i, video_id in enumerate(unique_videos_als_np)}\n",
    "num_videos_als = len(unique_videos_als_np)\n",
    "\n",
    "del unique_users_als_np, unique_videos_als_np\n",
    "gc.collect()\n",
    "\n",
    "als_user_ids = df_big_merged['user_id'].map(lambda x: user_to_idx.get(int(x)))\n",
    "als_item_ids = df_big_merged['video_id'].map(lambda x: video_to_idx.get(int(x)))\n",
    "\n",
    "# Check for NaNs introduced by .get() if some IDs were not in the mapping (should not happen if maps are from same df)\n",
    "if als_user_ids.isnull().any() or als_item_ids.isnull().any():\n",
    "    print(\"Warning: NaNs found in ALS mapped IDs. This indicates an issue with ID mapping.\")\n",
    "    als_user_ids = als_user_ids.fillna(-1).astype(int) # Handle potential NaNs, though ideally should not occur\n",
    "    als_item_ids = als_item_ids.fillna(-1).astype(int)\n",
    "\n",
    "als_ratings = df_big_merged['watch_ratio'].astype('float32')\n",
    "als_ratings_clipped = np.maximum(als_ratings, 0.001)\n",
    "\n",
    "interaction_matrix_als = csr_matrix((als_ratings_clipped, (als_user_ids, als_item_ids)),\n",
    "                                    shape=(num_users_als, num_videos_als))\n",
    "print(f\"ALS Sparse Matrix Shape: {interaction_matrix_als.shape}\")\n",
    "\n",
    "with open(os.path.join(PROCESSED_DATA_PATH, 'user_to_idx_als.json'), 'w') as f: json.dump(user_to_idx, f)\n",
    "with open(os.path.join(PROCESSED_DATA_PATH, 'video_to_idx_als.json'), 'w') as f: json.dump(video_to_idx, f)\n",
    "with open(os.path.join(PROCESSED_DATA_PATH, 'idx_to_user_als.json'), 'w') as f: json.dump(idx_to_user, f)\n",
    "with open(os.path.join(PROCESSED_DATA_PATH, 'idx_to_video_als.json'), 'w') as f: json.dump(idx_to_video, f)\n",
    "del als_user_ids, als_item_ids, als_ratings, als_ratings_clipped\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c293b718-c844-4872-b38a-56835b78ea38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 1.6: Define Train/Test Split (from Big Matrix) & Prepare LightGBM Data ---\n",
      "LGBM Train data shape (from big matrix): (10024644, 48)\n",
      "LGBM Test data shape (from big matrix): (2506162, 48)\n",
      "Processing categoricals for train_lgbm...\n",
      "Processing categoricals for test_lgbm...\n",
      "Processing categoricals for small_eval...\n",
      "Final categorical features for LightGBM: ['user_id', 'video_id', 'user_active_degree', 'interaction_hour', 'interaction_day_of_week', 'is_lowactive_period', 'is_live_streamer', 'is_video_author', 'onehot_feat0', 'onehot_feat1', 'onehot_feat2', 'onehot_feat3', 'onehot_feat4', 'onehot_feat5', 'onehot_feat6', 'onehot_feat7', 'onehot_feat8', 'onehot_feat9', 'onehot_feat10', 'onehot_feat11', 'onehot_feat12', 'onehot_feat13', 'onehot_feat14', 'onehot_feat15', 'onehot_feat16', 'onehot_feat17', 'author_id', 'video_type', 'video_tag_id']\n",
      "Final numerical features for LightGBM: ['video_duration_interaction', 'interaction_date', 'follow_user_num', 'fans_user_num', 'register_days', 'num_item_tags', 'video_duration_daily', 'daily_play_progress', 'daily_play_per_show_ratio', 'daily_like_per_play_ratio', 'daily_completion_rate', 'video_age_days']\n",
      "Total features for LightGBM: 41\n",
      "\n",
      "Saving processed LightGBM training, testing, and small_matrix evaluation data...\n",
      "Data preparation and saving complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1.6: Define Train/Test Split for LightGBM (from df_big_merged) & Save All Processed Data ---\n",
    "print(\"\\n--- Step 1.6: Define Train/Test Split (from Big Matrix) & Prepare LightGBM Data ---\")\n",
    "split_point = int(len(df_big_merged) * 0.8)\n",
    "train_df_lgbm = df_big_merged.iloc[:split_point].copy()\n",
    "test_df_lgbm = df_big_merged.iloc[split_point:].copy()\n",
    "del df_big_merged\n",
    "gc.collect()\n",
    "\n",
    "print(f\"LGBM Train data shape (from big matrix): {train_df_lgbm.shape}\")\n",
    "print(f\"LGBM Test data shape (from big matrix): {test_df_lgbm.shape}\")\n",
    "\n",
    "target_col = 'watch_ratio'\n",
    "cols_to_drop_lgbm = [\n",
    "    'time', 'timestamp', 'interaction_datetime', 'upload_dt', 'upload_dt_parsed',\n",
    "    'item_stats_date', 'play_duration'\n",
    "]\n",
    "# Define categorical_features_lgbm dynamically based on what's available and intended\n",
    "# Start with known categoricals\n",
    "categorical_features_lgbm = ['user_id', 'video_id', 'user_active_degree', 'interaction_hour', 'interaction_day_of_week']\n",
    "# Add categoricals from user_features\n",
    "user_cat_flags = ['is_lowactive_period', 'is_live_streamer', 'is_video_author'] # these were made int8\n",
    "for flag in user_cat_flags:\n",
    "    if flag in train_df_lgbm.columns: categorical_features_lgbm.append(flag)\n",
    "categorical_features_lgbm.extend(onehot_feature_names) # these are int16\n",
    "# Add categoricals from item_daily_features\n",
    "daily_cat_features = ['author_id', 'video_type', 'video_tag_id'] # video_tag_id added back\n",
    "for feat in daily_cat_features:\n",
    "    if feat in train_df_lgbm.columns: categorical_features_lgbm.append(feat)\n",
    "\n",
    "# Ensure all categorical features are actually present and correctly typed\n",
    "final_lgbm_categorical_features = []\n",
    "dfs_to_prep_cats = {'train_lgbm': train_df_lgbm, 'test_lgbm': test_df_lgbm, 'small_eval': df_small_eval_features}\n",
    "for df_name, df_curr in dfs_to_prep_cats.items():\n",
    "    print(f\"Processing categoricals for {df_name}...\")\n",
    "    for col in categorical_features_lgbm: # Iterate over the master list\n",
    "        if col in df_curr.columns:\n",
    "            if df_curr[col].dtype.name != 'category':\n",
    "                if df_curr[col].isnull().any():\n",
    "                    if col in ['user_active_degree', 'video_type']:\n",
    "                         if hasattr(df_curr[col], 'cat') and 'Unknown_Merge_NA' not in df_curr[col].cat.categories:\n",
    "                             df_curr[col] = df_curr[col].cat.add_categories(\"Unknown_Merge_NA\")\n",
    "                         df_curr[col] = df_curr[col].fillna(\"Unknown_Merge_NA\")\n",
    "                    else: # ID-like or onehot indices\n",
    "                         df_curr[col] = df_curr[col].fillna(-1) # Fill before astype\n",
    "                df_curr[col] = df_curr[col].astype('category')\n",
    "            if df_name == 'train_lgbm' and col not in final_lgbm_categorical_features : # Build final list from train_df\n",
    "                final_lgbm_categorical_features.append(col)\n",
    "        # else:\n",
    "            # print(f\"Info: Categorical feature '{col}' defined but not found in {df_name}.\")\n",
    "\n",
    "# Identify numerical features dynamically\n",
    "all_cols_train = list(train_df_lgbm.columns)\n",
    "numerical_features_lgbm = [\n",
    "    col for col in all_cols_train\n",
    "    if col not in final_lgbm_categorical_features and \\\n",
    "       col != target_col and \\\n",
    "       col not in cols_to_drop_lgbm\n",
    "]\n",
    "lgbm_feature_columns = final_lgbm_categorical_features + numerical_features_lgbm\n",
    "print(f\"Final categorical features for LightGBM: {final_lgbm_categorical_features}\")\n",
    "print(f\"Final numerical features for LightGBM: {numerical_features_lgbm}\")\n",
    "print(f\"Total features for LightGBM: {len(lgbm_feature_columns)}\")\n",
    "\n",
    "print(\"\\nSaving processed LightGBM training, testing, and small_matrix evaluation data...\")\n",
    "X_train = train_df_lgbm[lgbm_feature_columns]\n",
    "y_train = train_df_lgbm[target_col]\n",
    "pd.concat([X_train, y_train.rename(target_col)], axis=1).to_parquet(os.path.join(PROCESSED_DATA_PATH, 'lightgbm_train_data.parquet'), index=False)\n",
    "del X_train, y_train, train_df_lgbm\n",
    "gc.collect()\n",
    "\n",
    "X_test = test_df_lgbm[lgbm_feature_columns]\n",
    "y_test = test_df_lgbm[target_col]\n",
    "pd.concat([X_test, y_test.rename(target_col)], axis=1).to_parquet(os.path.join(PROCESSED_DATA_PATH, 'lightgbm_test_data.parquet'), index=False)\n",
    "test_df_lgbm[['user_id', 'video_id', target_col]].to_csv(os.path.join(PROCESSED_DATA_PATH, 'ground_truth_test_big_matrix.csv'), index=False)\n",
    "del X_test, y_test, test_df_lgbm\n",
    "gc.collect()\n",
    "\n",
    "# For df_small_eval_features, ensure it has all lgbm_feature_columns and the target_col\n",
    "# It might be missing columns if some features were not applicable or all NaN for small matrix data\n",
    "small_eval_cols_present = [col for col in lgbm_feature_columns if col in df_small_eval_features.columns]\n",
    "if target_col in df_small_eval_features.columns:\n",
    "    small_eval_cols_present.append(target_col)\n",
    "\n",
    "df_small_eval_features[small_eval_cols_present].to_parquet(os.path.join(PROCESSED_DATA_PATH, 'small_matrix_eval_features_data.parquet'), index=False)\n",
    "print(\"Data preparation and saving complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e47128ac-eac9-4217-9333-731ee8f9eb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Phase 2: Model Training ---\n",
      "Training ALS Model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cff4ec7a7344cc7a24fa2ce6ddc58ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS model saved to: ../models/als_model.joblib\n",
      "\n",
      "Training LightGBM Model...\n",
      "LightGBM model saved to: ../models/lightgbm_ranker_model.txt\n",
      "\n",
      "--- Model Training Complete ---\n"
     ]
    }
   ],
   "source": [
    "# --- Phase 2: Model Training ---\n",
    "print(\"\\n--- Phase 2: Model Training ---\")\n",
    "import implicit\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "\n",
    "print(\"Training ALS Model...\")\n",
    "als_model = implicit.als.AlternatingLeastSquares(factors=100, regularization=0.1, iterations=20, use_cg=True, calculate_training_loss=True)\n",
    "als_model.fit(interaction_matrix_als)\n",
    "joblib.dump(als_model, os.path.join(MODELS_PATH, \"als_model.joblib\"))\n",
    "print(f\"ALS model saved to: {os.path.join(MODELS_PATH, 'als_model.joblib')}\")\n",
    "del interaction_matrix_als # Free up ALS matrix\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\nTraining LightGBM Model...\")\n",
    "train_lgbm_loaded_df = pd.read_parquet(os.path.join(PROCESSED_DATA_PATH, 'lightgbm_train_data.parquet'))\n",
    "y_train_lgbm_loaded = train_lgbm_loaded_df[target_col]\n",
    "# X_train_lgbm_loaded needs to use the dynamically determined lgbm_feature_columns\n",
    "# However, lgbm_feature_columns was defined in the previous cell. We need to ensure it's available\n",
    "# or reload it. For simplicity, assume it's available or re-derive if necessary.\n",
    "# For now, just drop target. The saved parquet should only have these columns.\n",
    "X_train_lgbm_loaded = train_lgbm_loaded_df.drop(columns=[target_col])\n",
    "del train_lgbm_loaded_df\n",
    "gc.collect()\n",
    "\n",
    "# final_lgbm_categorical_features was also defined in the previous cell.\n",
    "# For robustness, re-check dtypes. Parquet usually preserves category well.\n",
    "categorical_for_lgbm_dataset = []\n",
    "for col in final_lgbm_categorical_features: # Use the list built from train_df previously\n",
    "    if col in X_train_lgbm_loaded.columns:\n",
    "        if X_train_lgbm_loaded[col].dtype.name != 'category':\n",
    "            X_train_lgbm_loaded[col] = X_train_lgbm_loaded[col].astype('category')\n",
    "        categorical_for_lgbm_dataset.append(col)\n",
    "\n",
    "\n",
    "lgbm_params_config = {\n",
    "    'objective': 'regression_l1', 'metric': ['mae', 'rmse'], 'boosting_type': 'gbdt',\n",
    "    'num_leaves': 63, 'learning_rate': 0.05, 'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8, 'bagging_freq': 1, 'n_estimators': 1000,\n",
    "    'verbose': -1, 'n_jobs': -1, 'seed': 42\n",
    "}\n",
    "\n",
    "lgb_train_dataset = lgb.Dataset(X_train_lgbm_loaded, y_train_lgbm_loaded,\n",
    "                                categorical_feature=categorical_for_lgbm_dataset, # Use the refined list\n",
    "                                free_raw_data=False)\n",
    "\n",
    "params_for_training = lgbm_params_config.copy()\n",
    "num_boost_round_from_params = params_for_training.pop('n_estimators', 1000)\n",
    "\n",
    "model_lgbm_trained = lgb.train(\n",
    "    params=params_for_training,\n",
    "    train_set=lgb_train_dataset,\n",
    "    num_boost_round=num_boost_round_from_params\n",
    ")\n",
    "model_lgbm_trained.save_model(os.path.join(MODELS_PATH, \"lightgbm_ranker_model.txt\"))\n",
    "print(f\"LightGBM model saved to: {os.path.join(MODELS_PATH, 'lightgbm_ranker_model.txt')}\")\n",
    "del X_train_lgbm_loaded, y_train_lgbm_loaded, lgb_train_dataset, model_lgbm_trained\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\n--- Model Training Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9635de0e-e3da-4a37-a55f-11d3131243a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Phase 3: Model Evaluation on Small Matrix ---\n",
      "Loading LightGBM model...\n",
      "Model loaded.\n",
      "\n",
      "Loading small matrix evaluation data (features + target)...\n",
      "Loaded small matrix evaluation data: (4676570, 42)\n",
      "Columns in df_small_eval: ['user_id', 'video_id', 'user_active_degree', 'interaction_hour', 'interaction_day_of_week', 'is_lowactive_period', 'is_live_streamer', 'is_video_author', 'onehot_feat0', 'onehot_feat1', 'onehot_feat2', 'onehot_feat3', 'onehot_feat4', 'onehot_feat5', 'onehot_feat6', 'onehot_feat7', 'onehot_feat8', 'onehot_feat9', 'onehot_feat10', 'onehot_feat11', 'onehot_feat12', 'onehot_feat13', 'onehot_feat14', 'onehot_feat15', 'onehot_feat16', 'onehot_feat17', 'author_id', 'video_type', 'video_tag_id', 'video_duration_interaction', 'interaction_date', 'follow_user_num', 'fans_user_num', 'register_days', 'num_item_tags', 'video_duration_daily', 'daily_play_progress', 'daily_play_per_show_ratio', 'daily_like_per_play_ratio', 'daily_completion_rate', 'video_age_days', 'watch_ratio']\n",
      "\n",
      "Verifying and casting categorical features in evaluation data...\n",
      "Number of features for evaluation: 41\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Assuming utils.py is in ../scripts/\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../scripts')))\n",
    "from utils import precision_at_k, recall_at_k, ndcg_at_k\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "PROCESSED_DATA_PATH = \"../data/\"\n",
    "MODELS_PATH = \"../models/\"\n",
    "TARGET_COL = 'watch_ratio'\n",
    "K_VALUES = [5, 10, 20, 50, 100, 250]\n",
    "RELEVANCE_THRESHOLD = 1.0 # Watch ratio > this threshold means relevant for ranking metrics\n",
    "\n",
    "print(f\"\\n--- Phase 3: Model Evaluation on Small Matrix ---\")\n",
    "\n",
    "# --- Step 3.1: Load Trained Model and Evaluation Data ---\n",
    "print(\"Loading LightGBM model...\")\n",
    "lgbm_model_path = os.path.join(MODELS_PATH, \"lightgbm_ranker_model.txt\")\n",
    "model_lgbm_loaded = lgb.Booster(model_file=lgbm_model_path)\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "print(f\"\\nLoading small matrix evaluation data (features + target)...\")\n",
    "small_matrix_eval_df_path = os.path.join(PROCESSED_DATA_PATH, 'small_matrix_eval_features_data.parquet')\n",
    "df_small_eval = pd.read_parquet(small_matrix_eval_df_path)\n",
    "print(f\"Loaded small matrix evaluation data: {df_small_eval.shape}\")\n",
    "print(f\"Columns in df_small_eval: {df_small_eval.columns.tolist()}\")\n",
    "\n",
    "# Identify feature columns (these should be all columns except the target)\n",
    "# It's crucial that these match the features the model was trained on.\n",
    "# The parquet file was saved with features + target.\n",
    "lgbm_feature_columns_eval = [col for col in df_small_eval.columns if col != TARGET_COL]\n",
    "\n",
    "# Ensure categorical features are correctly typed for prediction\n",
    "# We need the list of categorical feature names used during training.\n",
    "# This list should be consistent with what was defined in Step 1.6 of data prep.\n",
    "# For this script, let's redefine it or ensure it's loaded/passed correctly.\n",
    "# Assuming it was saved or can be reconstructed (as in the data prep script)\n",
    "\n",
    "# Reconstruct categorical feature list (should match training)\n",
    "onehot_feature_names_eval = [f'onehot_feat{i}' for i in range(18)]\n",
    "categorical_features_lgbm_eval = ['user_id', 'video_id', 'user_active_degree', 'interaction_hour', 'interaction_day_of_week']\n",
    "user_cat_flags_eval = ['is_lowactive_period', 'is_live_streamer', 'is_video_author']\n",
    "for flag in user_cat_flags_eval:\n",
    "    if flag in df_small_eval.columns: categorical_features_lgbm_eval.append(flag)\n",
    "categorical_features_lgbm_eval.extend(onehot_feature_names_eval)\n",
    "daily_cat_features_eval = ['author_id', 'video_type', 'video_tag_id']\n",
    "for feat in daily_cat_features_eval:\n",
    "    if feat in df_small_eval.columns: categorical_features_lgbm_eval.append(feat)\n",
    "\n",
    "final_categorical_for_eval = []\n",
    "print(\"\\nVerifying and casting categorical features in evaluation data...\")\n",
    "for col in categorical_features_lgbm_eval:\n",
    "    if col in df_small_eval.columns:\n",
    "        if df_small_eval[col].dtype.name != 'category':\n",
    "            # Fill NA before astype if any (should have been handled in data prep, but as a safeguard)\n",
    "            if df_small_eval[col].isnull().any():\n",
    "                if col in ['user_active_degree', 'video_type']:\n",
    "                     # df_small_eval[col] = df_small_eval[col].cat.add_categories(\"Unknown_Eval_NA\") # This line can cause error if not category yet\n",
    "                     df_small_eval[col] = df_small_eval[col].astype(str).fillna(\"Unknown_Eval_NA\") # Convert to str first\n",
    "                else:\n",
    "                     df_small_eval[col] = df_small_eval[col].fillna(-1)\n",
    "            df_small_eval[col] = df_small_eval[col].astype('category')\n",
    "        final_categorical_for_eval.append(col) # Keep track of actual categoricals found\n",
    "    # else:\n",
    "        # print(f\"Info: Categorical feature '{col}' defined but not found in small_eval_df.\")\n",
    "\n",
    "# Ensure lgbm_feature_columns_eval aligns with what's in df_small_eval\n",
    "lgbm_feature_columns_eval = [col for col in lgbm_feature_columns_eval if col in df_small_eval.columns]\n",
    "print(f\"Number of features for evaluation: {len(lgbm_feature_columns_eval)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c128b0ac-ff54-444a-8399-c9ec09b27e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Making predictions on small matrix data...\n",
      "Predictions complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Step 3.2: Make Predictions ---\n",
    "print(\"\\nMaking predictions on small matrix data...\")\n",
    "X_small_eval_features = df_small_eval[lgbm_feature_columns_eval]\n",
    "actual_watch_ratios = df_small_eval[TARGET_COL]\n",
    "\n",
    "predicted_watch_ratios = model_lgbm_loaded.predict(X_small_eval_features)\n",
    "df_small_eval['predicted_watch_ratio'] = predicted_watch_ratios\n",
    "print(\"Predictions complete.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b793d7a6-656d-43ba-9d44-fe3ebc492e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Pointwise Evaluation (Overall) ---\n",
      "Overall RMSE on Small Matrix: 1.3214\n",
      "Overall MAE on Small Matrix:  0.3473\n"
     ]
    }
   ],
   "source": [
    "# --- Step 3.3: Pointwise Evaluation Metrics (RMSE, MAE) ---\n",
    "print(f\"\\n--- Pointwise Evaluation (Overall) ---\")\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "rmse_overall = np.sqrt(mean_squared_error(actual_watch_ratios, predicted_watch_ratios))\n",
    "mae_overall = mean_absolute_error(actual_watch_ratios, predicted_watch_ratios)\n",
    "\n",
    "print(f\"Overall RMSE on Small Matrix: {rmse_overall:.4f}\")\n",
    "print(f\"Overall MAE on Small Matrix:  {mae_overall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "faab8591-a6c1-426b-9e1b-a86de525f000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Ranking Evaluation (Per User, then Averaged) ---\n",
      "Using relevance threshold: watch_ratio > 1.0\n",
      "Evaluating ranking for 1411 users...\n",
      "Processed 200/1411 users for ranking metrics...\n",
      "Processed 400/1411 users for ranking metrics...\n",
      "Processed 600/1411 users for ranking metrics...\n",
      "Processed 800/1411 users for ranking metrics...\n",
      "Processed 1000/1411 users for ranking metrics...\n",
      "Processed 1200/1411 users for ranking metrics...\n",
      "Processed 1400/1411 users for ranking metrics...\n",
      "Calculating average ranking metrics...\n",
      "\n",
      "Metrics for K=5:\n",
      "  Avg Precision@5: 0.9137\n",
      "  Avg Recall@5:    0.0048\n",
      "  Avg nDCG@5:      0.9146\n",
      "\n",
      "Metrics for K=10:\n",
      "  Avg Precision@10: 0.9117\n",
      "  Avg Recall@10:    0.0095\n",
      "  Avg nDCG@10:      0.9128\n",
      "\n",
      "Metrics for K=20:\n",
      "  Avg Precision@20: 0.9019\n",
      "  Avg Recall@20:    0.0188\n",
      "  Avg nDCG@20:      0.9057\n",
      "\n",
      "Metrics for K=50:\n",
      "  Avg Precision@50: 0.8571\n",
      "  Avg Recall@50:    0.0443\n",
      "  Avg nDCG@50:      0.8706\n",
      "\n",
      "Metrics for K=100:\n",
      "  Avg Precision@100: 0.8234\n",
      "  Avg Recall@100:    0.0848\n",
      "  Avg nDCG@100:      0.8396\n",
      "\n",
      "Metrics for K=250:\n",
      "  Avg Precision@250: 0.7725\n",
      "  Avg Recall@250:    0.1973\n",
      "  Avg nDCG@250:      0.7904\n",
      "\n",
      "--- Evaluation on Small Matrix Complete ---\n"
     ]
    }
   ],
   "source": [
    "# --- Step 3.4: Ranking Evaluation Metrics (Per User, then Averaged) ---\n",
    "print(f\"\\n--- Ranking Evaluation (Per User, then Averaged) ---\")\n",
    "print(f\"Using relevance threshold: watch_ratio > {RELEVANCE_THRESHOLD}\")\n",
    "\n",
    "all_user_metrics = {k: {'precision': [], 'recall': [], 'ndcg': []} for k in K_VALUES}\n",
    "unique_users_in_small_matrix = df_small_eval['user_id'].unique()\n",
    "num_eval_users = len(unique_users_in_small_matrix)\n",
    "\n",
    "print(f\"Evaluating ranking for {num_eval_users} users...\")\n",
    "\n",
    "# Group data by user_id for per-user evaluation\n",
    "grouped_small_eval = df_small_eval.groupby('user_id', observed=False)\n",
    "\n",
    "for i, (user_id, user_data) in enumerate(grouped_small_eval):\n",
    "    if (i + 1) % 200 == 0: # Print progress\n",
    "        print(f\"Processed {i+1}/{num_eval_users} users for ranking metrics...\")\n",
    "\n",
    "    # Sort items by predicted watch_ratio for this user (descending)\n",
    "    user_pred_items_sorted = user_data.sort_values(by='predicted_watch_ratio', ascending=False)['video_id'].tolist()\n",
    "    \n",
    "    # Get true relevant items for this user\n",
    "    user_true_relevant_items = set(user_data[user_data[TARGET_COL] > RELEVANCE_THRESHOLD]['video_id'].tolist())\n",
    "\n",
    "    if not user_true_relevant_items: # Skip user if they have no relevant items by our definition\n",
    "        continue\n",
    "\n",
    "    for k in K_VALUES:\n",
    "        p_at_k = precision_at_k(user_true_relevant_items, user_pred_items_sorted, k)\n",
    "        r_at_k = recall_at_k(user_true_relevant_items, user_pred_items_sorted, k)\n",
    "        n_at_k = ndcg_at_k(user_true_relevant_items, user_pred_items_sorted, k)\n",
    "        \n",
    "        all_user_metrics[k]['precision'].append(p_at_k)\n",
    "        all_user_metrics[k]['recall'].append(r_at_k)\n",
    "        all_user_metrics[k]['ndcg'].append(n_at_k)\n",
    "\n",
    "print(\"Calculating average ranking metrics...\")\n",
    "avg_metrics_report = {}\n",
    "for k in K_VALUES:\n",
    "    avg_precision = np.mean(all_user_metrics[k]['precision']) if all_user_metrics[k]['precision'] else 0\n",
    "    avg_recall = np.mean(all_user_metrics[k]['recall']) if all_user_metrics[k]['recall'] else 0\n",
    "    avg_ndcg = np.mean(all_user_metrics[k]['ndcg']) if all_user_metrics[k]['ndcg'] else 0\n",
    "    \n",
    "    avg_metrics_report[f\"Precision@{k}\"] = avg_precision\n",
    "    avg_metrics_report[f\"Recall@{k}\"] = avg_recall\n",
    "    avg_metrics_report[f\"nDCG@{k}\"] = avg_ndcg\n",
    "    \n",
    "    print(f\"\\nMetrics for K={k}:\")\n",
    "    print(f\"  Avg Precision@{k}: {avg_precision:.4f}\")\n",
    "    print(f\"  Avg Recall@{k}:    {avg_recall:.4f}\")\n",
    "    print(f\"  Avg nDCG@{k}:      {avg_ndcg:.4f}\")\n",
    "\n",
    "print(\"\\n--- Evaluation on Small Matrix Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca6a904-ae5f-484a-aec0-4e0b7e3f0f03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (implicit-env)",
   "language": "python",
   "name": "implicit-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
