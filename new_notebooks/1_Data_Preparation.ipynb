{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d030f9a4",
   "metadata": {},
   "source": [
    "# KRAFT: Data Preparation and Feature Engineering\n",
    "\n",
    "This notebook covers the initial stage of building our recommender system. The primary goals are:\n",
    "1. Load the raw KuaiRec dataset files.\n",
    "2. Perform initial cleaning, type conversions, and handle missing values.\n",
    "3. Engineer new features from the existing data to enrich our interaction logs.\n",
    "4. Prepare and save processed datasets for:\n",
    "    - Training an ALS model for candidate generation (from `big_matrix`).\n",
    "    - Training a LightGBM model for ranking (from `big_matrix`).\n",
    "    - Evaluating the ranker on a dense subset (`small_matrix`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfa80d2",
   "metadata": {},
   "source": [
    "## 1. Imports and Configuration\n",
    "\n",
    "First, we import necessary libraries and define global configurations such as file paths, column selections, and data types for initial loading and final processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00b441b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from scipy.sparse import csr_matrix\n",
    "import json\n",
    "import os\n",
    "\n",
    "RAW_DATA_BASE_PATH = \"../raw_data/KuaiRec/data/\"\n",
    "PROCESSED_DATA_PATH = \"../data/\"\n",
    "MODELS_PATH = \"../models/\"\n",
    "os.makedirs(PROCESSED_DATA_PATH, exist_ok=True)\n",
    "os.makedirs(MODELS_PATH, exist_ok=True)\n",
    "\n",
    "# --- Dtype Definitions & Column Selections ---\n",
    "\n",
    "# For interaction matrices (big_matrix.csv, small_matrix.csv)\n",
    "interaction_cols_initial_load = {\n",
    "    'user_id': 'float32', 'video_id': 'float32', 'play_duration': 'float32',\n",
    "    'video_duration': 'float32', 'time': 'str', 'date': 'float32',\n",
    "    'timestamp': 'float32', 'watch_ratio': 'float32'\n",
    "}\n",
    "interaction_cols_final_dtypes = {\n",
    "    'user_id': 'int32', 'video_id': 'int32', 'play_duration': 'int32',\n",
    "    'video_duration': 'int32', 'time': 'str', 'date': 'int32',\n",
    "    'timestamp': 'float32', 'watch_ratio': 'float32'\n",
    "}\n",
    "\n",
    "# For user_features.csv\n",
    "user_features_selected_cols = {\n",
    "    'user_id': 'int32', 'user_active_degree': 'category',\n",
    "    'is_lowactive_period': 'float32', \n",
    "    'is_live_streamer': 'float32', \n",
    "    'is_video_author': 'float32',\n",
    "    'follow_user_num': 'int32', \n",
    "    'fans_user_num': 'int32', \n",
    "    'register_days': 'int32',\n",
    "}\n",
    "onehot_feature_names = []\n",
    "for i in range(18):\n",
    "    col_name = f'onehot_feat{i}'\n",
    "    user_features_selected_cols[col_name] = 'float32' # Load as float to handle potential NaNs\n",
    "    onehot_feature_names.append(col_name)\n",
    "\n",
    "# For item_categories.csv\n",
    "item_categories_selected_cols = {'video_id': 'int32', 'feat': 'str'}\n",
    "\n",
    "# For item_daily_features.csv - initial load includes counts for ratio calculation\n",
    "item_daily_initial_load_cols = {\n",
    "    'video_id': 'float32', \n",
    "    'date': 'float32', # Will be renamed to item_stats_date\n",
    "    'author_id': 'float32',\n",
    "    'video_type': 'category', \n",
    "    'upload_dt': 'str', \n",
    "    'video_duration': 'float32', # Will be renamed to video_duration_daily\n",
    "    'show_cnt': 'float32', # For ratio\n",
    "    'play_cnt': 'float32', # For ratio\n",
    "    'like_cnt': 'float32', # For ratio\n",
    "    'complete_play_cnt': 'float32', # For ratio\n",
    "    'play_progress': 'float32', # Loaded as 'play_progress', will be 'daily_play_progress'\n",
    "    'video_tag_id': 'float32', # Daily item tag\n",
    "}\n",
    "# Final columns to keep from daily features for merging (after processing and ratio creation)\n",
    "item_daily_cols_to_keep_for_merge = [\n",
    "    'video_id', 'item_stats_date', 'author_id', 'video_type', 'upload_dt_parsed',\n",
    "    'video_duration_daily', 'daily_play_progress',\n",
    "    'daily_play_per_show_ratio', 'daily_like_per_play_ratio', 'daily_completion_rate',\n",
    "    'video_tag_id'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ff5257",
   "metadata": {},
   "source": [
    "## 2. Load and Pre-process Raw Data Files\n",
    "\n",
    "This section loads the primary data files (`big_matrix`, `small_matrix`, `user_features`, `item_categories`, `item_daily_features`). \n",
    "Initial processing includes:\n",
    "- Applying specified dtypes during loading.\n",
    "- Handling potential NaNs in integer columns by loading as float first, then filling NaNs and casting to the target integer type.\n",
    "- Parsing list-like strings (e.g., item tags).\n",
    "- Renaming columns for clarity and consistency (e.g., in `item_daily_features`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "670b5b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading big_matrix.csv...\n",
      "Loaded and processed big_matrix: (12530806, 8)\n",
      "\n",
      "Loading small_matrix.csv...\n",
      "Loaded and processed small_matrix: (4676570, 8)\n",
      "\n",
      "Loading user_features.csv (selected columns)...\n",
      "Loaded user_features: (7176, 26)\n",
      "\n",
      "Loading item_categories.csv...\n",
      "Loaded item_categories: (10728, 2)\n",
      "\n",
      "Loading and pre-processing item_daily_features.csv...\n",
      "Initial processing of item_daily_features complete. Shape: (343341, 12)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def post_process_interaction_df(df, final_dtypes_map):\n",
    "    \"\"\"Helper to fill NaNs and cast dtypes for interaction DataFrames.\"\"\"\n",
    "    int_cols_with_potential_na = ['user_id', 'video_id', 'play_duration', 'video_duration', 'date']\n",
    "    for col in int_cols_with_potential_na:\n",
    "        if col in df.columns:\n",
    "            fill_value = -1 if ('id' in col or 'date' in col) else 0\n",
    "            df[col] = df[col].fillna(fill_value).astype(final_dtypes_map[col])\n",
    "    for col in ['timestamp', 'watch_ratio']:\n",
    "        if col in df.columns: \n",
    "            df[col] = df[col].astype(final_dtypes_map[col])\n",
    "    return df\n",
    "\n",
    "print(\"Loading big_matrix.csv...\")\n",
    "df_big_interactions = pd.read_csv(os.path.join(RAW_DATA_BASE_PATH, \"big_matrix.csv\"),\n",
    "                                  usecols=interaction_cols_initial_load.keys(),\n",
    "                                  dtype=interaction_cols_initial_load)\n",
    "df_big_interactions = post_process_interaction_df(df_big_interactions, interaction_cols_final_dtypes)\n",
    "print(f\"Loaded and processed big_matrix: {df_big_interactions.shape}\")\n",
    "\n",
    "print(\"\\nLoading small_matrix.csv...\")\n",
    "df_small_interactions = pd.read_csv(os.path.join(RAW_DATA_BASE_PATH, \"small_matrix.csv\"),\n",
    "                                    usecols=interaction_cols_initial_load.keys(),\n",
    "                                    dtype=interaction_cols_initial_load)\n",
    "df_small_interactions = post_process_interaction_df(df_small_interactions, interaction_cols_final_dtypes)\n",
    "print(f\"Loaded and processed small_matrix: {df_small_interactions.shape}\")\n",
    "\n",
    "print(\"\\nLoading user_features.csv (selected columns)...\")\n",
    "df_user_features = pd.read_csv(os.path.join(RAW_DATA_BASE_PATH, \"user_features.csv\"),\n",
    "                               usecols=user_features_selected_cols.keys(),\n",
    "                               dtype=user_features_selected_cols)\n",
    "for col in onehot_feature_names:\n",
    "    df_user_features[col] = df_user_features[col].fillna(-1).astype('int16')\n",
    "for col in ['is_lowactive_period', 'is_live_streamer', 'is_video_author']:\n",
    "    if col in df_user_features.columns:\n",
    "        if df_user_features[col].isnull().any(): \n",
    "            df_user_features[col] = df_user_features[col].fillna(-1).astype('int8') # -1 for missing flag\n",
    "        else: \n",
    "            df_user_features[col] = df_user_features[col].astype('int8')\n",
    "print(f\"Loaded user_features: {df_user_features.shape}\")\n",
    "\n",
    "print(\"\\nLoading item_categories.csv...\")\n",
    "df_item_categories = pd.read_csv(os.path.join(RAW_DATA_BASE_PATH, \"item_categories.csv\"),\n",
    "                                 usecols=item_categories_selected_cols.keys(),\n",
    "                                 dtype=item_categories_selected_cols)\n",
    "def parse_feat_list(feat_str):\n",
    "    if isinstance(feat_str, list): return len(feat_str)\n",
    "    if pd.isna(feat_str) or not isinstance(feat_str, str): return 0\n",
    "    try: return len(eval(feat_str))\n",
    "    except: return 0 # Handles malformed strings\n",
    "df_item_categories['num_item_tags'] = df_item_categories['feat'].apply(parse_feat_list).astype('int16')\n",
    "df_item_categories = df_item_categories.drop(columns=['feat'])\n",
    "print(f\"Loaded item_categories: {df_item_categories.shape}\")\n",
    "\n",
    "print(\"\\nLoading and pre-processing item_daily_features.csv...\")\n",
    "df_item_daily_full_load = pd.read_csv(os.path.join(RAW_DATA_BASE_PATH, \"item_daily_features.csv\"),\n",
    "                                usecols=item_daily_initial_load_cols.keys(),\n",
    "                                dtype=item_daily_initial_load_cols)\n",
    "df_item_daily_processed = df_item_daily_full_load.rename(columns={\n",
    "    'date': 'item_stats_date', \n",
    "    'video_duration': 'video_duration_daily',\n",
    "    'show_cnt': 'daily_show_cnt', \n",
    "    'play_cnt': 'daily_play_cnt',\n",
    "    'like_cnt': 'daily_like_cnt', \n",
    "    'complete_play_cnt': 'daily_complete_play_cnt',\n",
    "    'play_progress': 'daily_play_progress'\n",
    "})\n",
    "del df_item_daily_full_load\n",
    "gc.collect()\n",
    "\n",
    "# NA fill and type conversion for the initially processed item_daily_features\n",
    "id_cols_daily = ['video_id', 'author_id', 'video_tag_id']\n",
    "count_cols_daily = ['daily_show_cnt', 'daily_play_cnt', 'daily_like_cnt', 'daily_complete_play_cnt']\n",
    "for col in id_cols_daily:\n",
    "    if col in df_item_daily_processed.columns:\n",
    "        df_item_daily_processed[col] = df_item_daily_processed[col].fillna(-1).astype('int32')\n",
    "if 'item_stats_date' in df_item_daily_processed.columns:\n",
    "    df_item_daily_processed['item_stats_date'] = df_item_daily_processed['item_stats_date'].fillna(-1).astype('int32')\n",
    "for col in count_cols_daily:\n",
    "    if col in df_item_daily_processed.columns:\n",
    "        df_item_daily_processed[col] = df_item_daily_processed[col].fillna(0).astype('int32')\n",
    "\n",
    "float_cols_daily = ['video_duration_daily', 'daily_play_progress']\n",
    "for col in float_cols_daily:\n",
    "    if col in df_item_daily_processed.columns:\n",
    "        df_item_daily_processed[col] = df_item_daily_processed[col].fillna(0).astype('float32')\n",
    "\n",
    "print(f\"Initial processing of item_daily_features complete. Shape: {df_item_daily_processed.shape}\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acd6a0d",
   "metadata": {},
   "source": [
    "## 3. Global Processing of Daily Item Features\n",
    "\n",
    "Here, we perform further transformations on the `item_daily_processed` DataFrame:\n",
    "- Parse `upload_dt` to datetime objects (`upload_dt_parsed`).\n",
    "- Calculate key ratios like play-per-show, like-per-play, and completion rate.\n",
    "- Ensure `video_type` is categorical.\n",
    "- Select only the essential columns for merging (`item_daily_cols_to_keep_for_merge`).\n",
    "- **Crucially, identify and remove duplicate entries based on `(video_id, item_stats_date)`** to prevent row explosion during merges. This was a key finding from previous iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70d40f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nGlobally processing df_item_daily for derived features & slimming...\")\n",
    "if 'upload_dt' in df_item_daily_processed.columns:\n",
    "    df_item_daily_processed['upload_dt_parsed'] = pd.to_datetime(df_item_daily_processed['upload_dt'], errors='coerce')\n",
    "else:\n",
    "    # If 'upload_dt' wasn't loaded (e.g., due to column selection changes), create a placeholder\n",
    "    df_item_daily_processed['upload_dt_parsed'] = pd.NaT \n",
    "\n",
    "# Calculate Ratios (with checks for column existence)\n",
    "for col_pair, new_col_name in [\n",
    "    (('daily_play_cnt', 'daily_show_cnt'), 'daily_play_per_show_ratio'),\n",
    "    (('daily_like_cnt', 'daily_play_cnt'), 'daily_like_per_play_ratio'),\n",
    "    (('daily_complete_play_cnt', 'daily_play_cnt'), 'daily_completion_rate')\n",
    "]:\n",
    "    num_col, den_col = col_pair\n",
    "    if num_col in df_item_daily_processed.columns and den_col in df_item_daily_processed.columns:\n",
    "        df_item_daily_processed[new_col_name] = (df_item_daily_processed[num_col] / (df_item_daily_processed[den_col] + 1e-6)).astype('float32')\n",
    "    else:\n",
    "        df_item_daily_processed[new_col_name] = np.float32(0.0) # Default if components are missing\n",
    "\n",
    "if 'video_type' in df_item_daily_processed.columns and df_item_daily_processed['video_type'].dtype.name != 'category':\n",
    "    df_item_daily_processed['video_type'] = df_item_daily_processed['video_type'].astype('category')\n",
    "    # Ensure 'Unknown' category exists if we plan to fill NaNs with it later\n",
    "    if 'Unknown' not in df_item_daily_processed['video_type'].cat.categories:\n",
    "        df_item_daily_processed['video_type'] = df_item_daily_processed['video_type'].cat.add_categories('Unknown')\n",
    "\n",
    "# Select final columns for the lean daily features table\n",
    "actual_cols_to_keep = [col for col in item_daily_cols_to_keep_for_merge if col in df_item_daily_processed.columns]\n",
    "df_item_daily_lean_for_merge = df_item_daily_processed[actual_cols_to_keep].copy()\n",
    "print(f\"Slimmed df_item_daily_lean_for_merge for merging. Shape: {df_item_daily_lean_for_merge.shape}, Columns: {df_item_daily_lean_for_merge.columns.tolist()}\")\n",
    "\n",
    "# Remove Duplicates from the lean daily features table\n",
    "print(\"\\nChecking for duplicate keys in df_item_daily_lean_for_merge...\")\n",
    "key_cols_daily = ['video_id', 'item_stats_date']\n",
    "actual_key_cols_daily = [col for col in key_cols_daily if col in df_item_daily_lean_for_merge.columns]\n",
    "\n",
    "if len(actual_key_cols_daily) == len(key_cols_daily):\n",
    "    duplicate_daily_keys = df_item_daily_lean_for_merge.duplicated(subset=actual_key_cols_daily, keep=False)\n",
    "    num_duplicate_daily_keys = duplicate_daily_keys.sum()\n",
    "    print(f\"Number of rows involved in duplicate ({', '.join(actual_key_cols_daily)}) keys: {num_duplicate_daily_keys}\")\n",
    "    if num_duplicate_daily_keys > 0:\n",
    "        # print(\"Example duplicate key entries:\") # Optional: view duplicates\n",
    "        # print(df_item_daily_lean_for_merge[duplicate_daily_keys].sort_values(by=actual_key_cols_daily).head())\n",
    "        print(\"Attempting to drop duplicate keys from df_item_daily_lean_for_merge, keeping first...\")\n",
    "        df_item_daily_lean_for_merge = df_item_daily_lean_for_merge.drop_duplicates(subset=actual_key_cols_daily, keep='first').copy()\n",
    "        print(f\"Shape of df_item_daily_lean_for_merge after dropping duplicates: {df_item_daily_lean_for_merge.shape}\")\n",
    "else:\n",
    "    print(f\"Warning: One or more key columns for daily features duplicate check ({key_cols_daily}) not found. Skipping duplicate check.\")\n",
    "\n",
    "del df_item_daily_processed # Free memory from the intermediate processed df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb10fb6",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering Function for Interactions\n",
    "\n",
    "This function, `engineer_features_for_interactions`, consolidates all steps to transform a raw interaction DataFrame (either `big_matrix` or `small_matrix` based) into a feature-rich DataFrame ready for modeling. It performs:\n",
    "- Extraction of temporal features (hour, day of week).\n",
    "- Merging with user features, static item features (categories), and the processed (lean) daily item features.\n",
    "- Post-merge feature creation (e.g., `video_age_days`).\n",
    "- NA filling for columns introduced by merges.\n",
    "- Sorting by timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc9f297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features_for_interactions(df_interactions_raw, df_user_features_ref, df_item_categories_ref, df_item_daily_ref_lean):\n",
    "    print(f\"Engineering features for interaction table with shape: {df_interactions_raw.shape}\")\n",
    "    df = df_interactions_raw.copy()\n",
    "\n",
    "    # Temporal features from interaction time\n",
    "    df['interaction_datetime'] = pd.to_datetime(df['time'], errors='coerce')\n",
    "    df['interaction_hour'] = df['interaction_datetime'].dt.hour.fillna(-1).astype('int8')\n",
    "    df['interaction_day_of_week'] = df['interaction_datetime'].dt.dayofweek.fillna(-1).astype('int8')\n",
    "    df = df.rename(columns={'date': 'interaction_date', 'video_duration': 'video_duration_interaction'})\n",
    "\n",
    "    # Merge with user features\n",
    "    print(\"Merging user features...\")\n",
    "    df = pd.merge(df, df_user_features_ref, on='user_id', how='left')\n",
    "    print(f\"Shape after user features merge: {df.shape}\")\n",
    "    gc.collect()\n",
    "    \n",
    "    # Merge with static item features\n",
    "    print(\"Merging static item features...\")\n",
    "    df = pd.merge(df, df_item_categories_ref, on='video_id', how='left')\n",
    "    df['num_item_tags'] = df['num_item_tags'].fillna(0).astype('int16')\n",
    "    print(f\"Shape after static item features merge: {df.shape}\")\n",
    "    gc.collect()\n",
    "\n",
    "    # Merge with lean dynamic item features\n",
    "    print(\"Merging dynamic item features (lean)...\")\n",
    "    df = pd.merge(df, df_item_daily_ref_lean,\n",
    "                     left_on=['video_id', 'interaction_date'],\n",
    "                     right_on=['video_id', 'item_stats_date'], # item_stats_date is the 'date' from daily features\n",
    "                     how='left')\n",
    "    print(f\"Shape after dynamic item features merge: {df.shape}\")\n",
    "    gc.collect()\n",
    "\n",
    "    # Post-merge feature engineering\n",
    "    print(\"Post-merge feature engineering...\")\n",
    "    if 'upload_dt_parsed' in df.columns and 'interaction_datetime' in df.columns:\n",
    "        video_age_delta = df['interaction_datetime'] - df['upload_dt_parsed']\n",
    "        df['video_age_days'] = video_age_delta.dt.days.fillna(-1).astype('int16')\n",
    "    else:\n",
    "        # Ensure series has same index if created manually\n",
    "        df['video_age_days'] = pd.Series([-1]*len(df), index=df.index, dtype='int16') \n",
    "\n",
    "    # NA filling for columns that came from the lean daily merge\n",
    "    # These are columns from item_daily_cols_to_keep_for_merge (excluding join keys & upload_dt_parsed)\n",
    "    cols_from_daily_merge_to_fill = [\n",
    "        'author_id', 'video_type', 'video_duration_daily', 'daily_play_progress',\n",
    "        'daily_play_per_show_ratio', 'daily_like_per_play_ratio', 'daily_completion_rate',\n",
    "        'video_tag_id'\n",
    "    ]\n",
    "    for col in cols_from_daily_merge_to_fill:\n",
    "        if col in df.columns:\n",
    "            if col in ['author_id', 'video_tag_id']:\n",
    "                df[col] = df[col].fillna(-1).astype('int32')\n",
    "            elif col == 'video_type':\n",
    "                if df[col].isnull().any(): # Only process if NaNs exist\n",
    "                    # Ensure it's category type first\n",
    "                    if df[col].dtype.name != 'category':\n",
    "                         df[col] = pd.Categorical(df[col]) # Convert if it lost category type\n",
    "                    # Add 'Unknown' category if not present\n",
    "                    if 'Unknown' not in df[col].cat.categories:\n",
    "                        df[col] = df[col].cat.add_categories('Unknown')\n",
    "                    df[col] = df[col].fillna('Unknown')\n",
    "            else: # Numerical daily features (durations, ratios, progress)\n",
    "                df[col] = df[col].fillna(0).astype('float32')\n",
    "        # else:\n",
    "            # print(f\"Warning: Column '{col}' expected from daily merge not found post-merge.\")\n",
    "            \n",
    "    df = df.sort_values(by='timestamp').reset_index(drop=True)\n",
    "    print(f\"Finished engineering. Shape: {df.shape}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05afaff",
   "metadata": {},
   "source": [
    "## 5. Apply Feature Engineering to Interaction Matrices\n",
    "\n",
    "Now, we use the defined `engineer_features_for_interactions` function to process both the `big_matrix` (for training/testing the main model) and the `small_matrix` (for dense evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df86ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Applying Feature Engineering to Big Matrix Interactions ---\")\n",
    "df_big_merged = engineer_features_for_interactions(df_big_interactions, df_user_features, df_item_categories, df_item_daily_lean_for_merge)\n",
    "del df_big_interactions\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\n--- Applying Feature Engineering to Small Matrix Interactions ---\")\n",
    "df_small_eval_features = engineer_features_for_interactions(df_small_interactions, df_user_features, df_item_categories, df_item_daily_lean_for_merge)\n",
    "del df_small_interactions\n",
    "gc.collect()\n",
    "\n",
    "# At this point, original feature DataFrames can be deleted if memory is critical\n",
    "# del df_user_features, df_item_categories, df_item_daily_lean_for_merge\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb40c8c",
   "metadata": {},
   "source": [
    "## 6. Prepare Data for ALS Candidate Generation\n",
    "\n",
    "The ALS model requires a user-item interaction matrix. We create this from `df_big_merged`:\n",
    "- Map original `user_id`s and `video_id`s to 0-based integer indices.\n",
    "- Use `watch_ratio` (clipped to be > 0) as the interaction strength/confidence.\n",
    "- Construct a sparse matrix (`csr_matrix`).\n",
    "- Save the ID mappings for later use (e.g., translating recommendations back to original IDs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c02a48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Preparing Data for ALS (Candidate Generation from Big Matrix) ---\")\n",
    "if 'user_id' not in df_big_merged.columns or 'video_id' not in df_big_merged.columns:\n",
    "    raise ValueError(\"user_id or video_id not found in df_big_merged for ALS prep.\")\n",
    "\n",
    "unique_users_als_np = df_big_merged['user_id'].unique()\n",
    "unique_videos_als_np = df_big_merged['video_id'].unique()\n",
    "\n",
    "user_to_idx = {int(user_id): i for i, user_id in enumerate(unique_users_als_np)}\n",
    "idx_to_user = {i: int(user_id) for i, user_id in enumerate(unique_users_als_np)} \n",
    "num_users_als = len(unique_users_als_np)\n",
    "\n",
    "video_to_idx = {int(video_id): i for i, video_id in enumerate(unique_videos_als_np)}\n",
    "idx_to_video = {i: int(video_id) for i, video_id in enumerate(unique_videos_als_np)}\n",
    "num_videos_als = len(unique_videos_als_np)\n",
    "\n",
    "del unique_users_als_np, unique_videos_als_np # Free numpy arrays\n",
    "gc.collect()\n",
    "\n",
    "als_user_ids = df_big_merged['user_id'].map(lambda x: user_to_idx.get(int(x)))\n",
    "als_item_ids = df_big_merged['video_id'].map(lambda x: video_to_idx.get(int(x)))\n",
    "\n",
    "# Check for NaNs that might arise if IDs in df_big_merged are not in user_to_idx/video_to_idx keys\n",
    "# This should ideally not happen if mappings are derived from the same df_big_merged.\n",
    "if als_user_ids.isnull().any() or als_item_ids.isnull().any():\n",
    "    print(\"Warning: NaNs found in ALS mapped IDs. Filling with -1. Investigate if unexpected.\")\n",
    "    # A row with user_id or item_id = -1 (our NA placeholder) would map to None via .get()\n",
    "    # These rows should be filtered out or handled if they are significant.\n",
    "    # For csr_matrix, indices must be non-negative. Let's filter them out.\n",
    "    valid_indices_mask = ~(als_user_ids.isnull() | als_item_ids.isnull())\n",
    "    als_user_ids = als_user_ids[valid_indices_mask]\n",
    "    als_item_ids = als_item_ids[valid_indices_mask]\n",
    "    als_ratings_source = df_big_merged['watch_ratio'][valid_indices_mask]\n",
    "    print(f\"Filtered out {len(df_big_merged) - len(als_ratings_source)} rows with invalid ALS IDs.\")\n",
    "else:\n",
    "    als_ratings_source = df_big_merged['watch_ratio']\n",
    "\n",
    "als_ratings_clipped = np.maximum(als_ratings_source.astype('float32'), 0.001)\n",
    "\n",
    "interaction_matrix_als = csr_matrix((als_ratings_clipped, (als_user_ids.astype(int), als_item_ids.astype(int))),\n",
    "                                    shape=(num_users_als, num_videos_als))\n",
    "print(f\"ALS Sparse Matrix Shape: {interaction_matrix_als.shape}, NNZ: {interaction_matrix_als.nnz}\")\n",
    "\n",
    "# Save ID mappings\n",
    "for mapping_dict, name in [\n",
    "    (user_to_idx, 'user_to_idx_als.json'), \n",
    "    (video_to_idx, 'video_to_idx_als.json'),\n",
    "    (idx_to_user, 'idx_to_user_als.json'),\n",
    "    (idx_to_video, 'idx_to_video_als.json')\n",
    "]:\n",
    "    with open(os.path.join(PROCESSED_DATA_PATH, name), 'w') as f: \n",
    "        json.dump(mapping_dict, f)\n",
    "print(f\"ALS ID mappings saved to {PROCESSED_DATA_PATH}\")\n",
    "\n",
    "del als_user_ids, als_item_ids, als_ratings_source, als_ratings_clipped\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72e1f10",
   "metadata": {},
   "source": [
    "## 7. Prepare Data for LightGBM Ranker and Save Processed Files\n",
    "\n",
    "This final data preparation step involves:\n",
    "- Splitting `df_big_merged` chronologically into training and testing sets for the LightGBM model.\n",
    "- Dynamically identifying categorical and numerical features based on the columns present in the processed DataFrames.\n",
    "- Ensuring all categorical features are correctly typed across `train_df_lgbm`, `test_df_lgbm`, and `df_small_eval_features`.\n",
    "- Saving the following files to disk:\n",
    "    - `lightgbm_train_data.parquet`: Training data for LightGBM (features + target from `big_matrix`).\n",
    "    - `lightgbm_test_data.parquet`: Test data for LightGBM (features + target from `big_matrix` holdout).\n",
    "    - `ground_truth_test_big_matrix.csv`: User-item pairs and actual `watch_ratio` for the `big_matrix` test set.\n",
    "    - `small_matrix_eval_features_data.parquet`: Fully processed `small_matrix` data (features + target) for dense evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2362fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Preparing Data for LightGBM Ranker & Saving Processed Files ---\")\n",
    "\n",
    "# Chronological split of df_big_merged for LightGBM training and standard testing\n",
    "split_point = int(len(df_big_merged) * 0.8)\n",
    "train_df_lgbm = df_big_merged.iloc[:split_point].copy()\n",
    "test_df_lgbm = df_big_merged.iloc[split_point:].copy()\n",
    "del df_big_merged # Free memory\n",
    "gc.collect()\n",
    "\n",
    "print(f\"LGBM Train data shape (from big matrix): {train_df_lgbm.shape}\")\n",
    "print(f\"LGBM Test data shape (from big matrix): {test_df_lgbm.shape}\")\n",
    "\n",
    "target_col = 'watch_ratio'\n",
    "cols_to_drop_for_lgbm_features = [\n",
    "    'time', 'timestamp', 'interaction_datetime', 'upload_dt', 'upload_dt_parsed',\n",
    "    'item_stats_date', 'play_duration'\n",
    "]\n",
    "\n",
    "# Dynamically define categorical features for LightGBM based on selected features\n",
    "base_categorical_features = ['user_id', 'video_id', 'user_active_degree', \n",
    "                             'interaction_hour', 'interaction_day_of_week']\n",
    "user_flag_categoricals = ['is_lowactive_period', 'is_live_streamer', 'is_video_author']\n",
    "daily_item_categoricals = ['author_id', 'video_type', 'video_tag_id']\n",
    "\n",
    "master_categorical_list = base_categorical_features + \\\n",
    "                            [flag for flag in user_flag_categoricals if flag in train_df_lgbm.columns] + \\\n",
    "                            onehot_feature_names + \\\n",
    "                            [feat for feat in daily_item_categoricals if feat in train_df_lgbm.columns]\n",
    "\n",
    "# Ensure categorical features are correctly typed in all DataFrames that will be used by LightGBM\n",
    "final_lgbm_categorical_features = [] # This will be the definitive list based on train_df_lgbm\n",
    "dfs_for_lgbm_prep = {\n",
    "    'train_lgbm': train_df_lgbm, \n",
    "    'test_lgbm': test_df_lgbm, \n",
    "    'small_eval': df_small_eval_features\n",
    "}\n",
    "\n",
    "for df_name, current_df in dfs_for_lgbm_prep.items():\n",
    "    print(f\"Processing categorical dtypes for {df_name}...\")\n",
    "    for col in master_categorical_list:\n",
    "        if col in current_df.columns:\n",
    "            if current_df[col].dtype.name != 'category':\n",
    "                # Fill NaNs that might have been introduced if a merge key was -1 (NA placeholder)\n",
    "                if current_df[col].isnull().any():\n",
    "                    if col in ['user_active_degree', 'video_type']:\n",
    "                        # Ensure 'Unknown_Merge_NA' can be added if it's already category\n",
    "                        if current_df[col].dtype.name == 'category' and 'Unknown_Merge_NA' not in current_df[col].cat.categories:\n",
    "                            current_df[col] = current_df[col].cat.add_categories(\"Unknown_Merge_NA\")\n",
    "                        current_df[col] = current_df[col].fillna(\"Unknown_Merge_NA\")\n",
    "                    else: # For ID-like features or onehot encoded features\n",
    "                         current_df[col] = current_df[col].fillna(-1)\n",
    "                current_df[col] = current_df[col].astype('category')\n",
    "            \n",
    "            # Build the final list of categorical features based on train_df_lgbm\n",
    "            if df_name == 'train_lgbm' and col not in final_lgbm_categorical_features:\n",
    "                final_lgbm_categorical_features.append(col)\n",
    "\n",
    "# Identify numerical features (all remaining columns not categorical, target, or to be dropped)\n",
    "all_columns_in_train = list(train_df_lgbm.columns)\n",
    "numerical_features_lgbm = [\n",
    "    col for col in all_columns_in_train\n",
    "    if col not in final_lgbm_categorical_features and \\\n",
    "       col != target_col and \\\n",
    "       col not in cols_to_drop_for_lgbm_features\n",
    "]\n",
    "lgbm_feature_columns = final_lgbm_categorical_features + numerical_features_lgbm\n",
    "\n",
    "print(f\"Final categorical features for LightGBM: {len(final_lgbm_categorical_features)} features\")\n",
    "print(f\"Final numerical features for LightGBM: {len(numerical_features_lgbm)} features\")\n",
    "print(f\"Total features for LightGBM: {len(lgbm_feature_columns)}\")\n",
    "\n",
    "# --- Save Processed DataFrames to Parquet/CSV ---\n",
    "print(\"\\nSaving processed LightGBM training, testing, and small_matrix evaluation data...\")\n",
    "\n",
    "# Training data (from big_matrix)\n",
    "X_train = train_df_lgbm[lgbm_feature_columns]\n",
    "y_train = train_df_lgbm[target_col]\n",
    "pd.concat([X_train, y_train.rename(target_col)], axis=1).to_parquet(\n",
    "    os.path.join(PROCESSED_DATA_PATH, 'lightgbm_train_data.parquet'), index=False\n",
    ")\n",
    "print(f\"Saved lightgbm_train_data.parquet: {X_train.shape[0]} rows, {X_train.shape[1]+1} cols\")\n",
    "del X_train, y_train, train_df_lgbm\n",
    "gc.collect()\n",
    "\n",
    "# Testing data (from big_matrix holdout)\n",
    "X_test = test_df_lgbm[lgbm_feature_columns]\n",
    "y_test = test_df_lgbm[target_col]\n",
    "pd.concat([X_test, y_test.rename(target_col)], axis=1).to_parquet(\n",
    "    os.path.join(PROCESSED_DATA_PATH, 'lightgbm_test_data.parquet'), index=False\n",
    ")\n",
    "print(f\"Saved lightgbm_test_data.parquet: {X_test.shape[0]} rows, {X_test.shape[1]+1} cols\")\n",
    "\n",
    "# Ground truth for the standard test set (big_matrix holdout)\n",
    "test_df_lgbm[['user_id', 'video_id', target_col]].to_csv(\n",
    "    os.path.join(PROCESSED_DATA_PATH, 'ground_truth_test_big_matrix.csv'), index=False\n",
    ")\n",
    "print(f\"Saved ground_truth_test_big_matrix.csv: {test_df_lgbm.shape[0]} rows\")\n",
    "del X_test, y_test, test_df_lgbm\n",
    "gc.collect()\n",
    "\n",
    "# Small matrix evaluation data (fully processed)\n",
    "# Ensure only relevant columns (features + target) are saved for small_eval_features\n",
    "small_eval_cols_to_save = [col for col in lgbm_feature_columns if col in df_small_eval_features.columns]\n",
    "if target_col in df_small_eval_features.columns:\n",
    "    small_eval_cols_to_save.append(target_col)\n",
    "else:\n",
    "    print(f\"Warning: Target column '{target_col}' not found in df_small_eval_features.\")\n",
    "\n",
    "df_small_eval_features[small_eval_cols_to_save].to_parquet(\n",
    "    os.path.join(PROCESSED_DATA_PATH, 'small_matrix_eval_features_data.parquet'), index=False\n",
    ")\n",
    "print(f\"Saved small_matrix_eval_features_data.parquet: {df_small_eval_features.shape[0]} rows, {len(small_eval_cols_to_save)} cols\")\n",
    "del df_small_eval_features\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\n--- Data Preparation and Feature Engineering Complete. Processed files are in ../data/ ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (implicit-env)",
   "language": "python",
   "name": "implicit-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
