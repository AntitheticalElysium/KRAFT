 {
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eval-md-intro",
   "metadata": {},
   "source": [
    "# 4. Evaluation & Submission\n",
    "\n",
    "**Goal:**\n",
    "1.  Load the trained LightGBM ranking model and associated components (encoders, embeddings, feature list).\n",
    "2.  Load the test user-item pairs (`interactions_test.csv`) and the necessary feature data.\n",
    "3.  Prepare the feature set for the test pairs, ensuring consistency with the training feature engineering.\n",
    "4.  Generate prediction scores using the LightGBM model.\n",
    "5.  Evaluate the model's ranking performance using standard metrics (Precision@K, Recall@K, NDCG@K) against the ground truth.\n",
    "6.  Generate the final `submission.csv` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-md-setup",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Import libraries, define paths, and load necessary evaluation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-code-imports-paths",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm # Progress bar\n",
    "import ast\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add scripts directory to path to import utils\n",
    "sys.path.append('../scripts/')\n",
    "try:\n",
    "    from utils import precision_at_k, recall_at_k, ndcg_at_k\n",
    "except ImportError:\n",
    "    print(\"ERROR: Could not import from scripts/utils.py. Ensure the file exists.\")\n",
    "    # Define metrics inline as fallback \n",
    "    def precision_at_k(y_true_items, y_pred_items, k):\n",
    "        pred_k = y_pred_items[:k]\n",
    "        relevant_in_pred_k = len(set(pred_k) & y_true_items)\n",
    "        return relevant_in_pred_k / k if k > 0 else 0\n",
    "    def recall_at_k(y_true_items, y_pred_items, k):\n",
    "        pred_k = y_pred_items[:k]\n",
    "        relevant_in_pred_k = len(set(pred_k) & y_true_items)\n",
    "        return relevant_in_pred_k / len(y_true_items) if len(y_true_items) > 0 else 0\n",
    "    def ndcg_at_k(y_true_items, y_pred_items, k):\n",
    "        pred_k = y_pred_items[:k]; dcg = 0.0\n",
    "        for i, item in enumerate(pred_k):\n",
    "            if item in y_true_items: dcg += 1.0 / np.log2(i + 2)\n",
    "        idcg = 0.0; num_relevant = min(len(y_true_items), k)\n",
    "        for i in range(num_relevant): idcg += 1.0 / np.log2(i + 2)\n",
    "        return dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_DIR = \"../data/\"\n",
    "RAW_DATA_DIR = \"../raw_data/KuaiRec/data/\"\n",
    "MODEL_DIR = \"../models/\"\n",
    "SUBMISSION_PATH = \"../submission.csv\"\n",
    "\n",
    "# Input file paths\n",
    "INTERACTIONS_TEST_PATH = os.path.join(DATA_DIR, \"interactions_test.csv\")\n",
    "GROUND_TRUTH_PATH = os.path.join(DATA_DIR, \"test_user_item_map.pkl\")\n",
    "USER_FEATURES_PATH = os.path.join(RAW_DATA_DIR, \"user_features.csv\")\n",
    "VIDEO_METADATA_PATH = os.path.join(DATA_DIR, \"video_metadata.csv\")\n",
    "INTERACTIONS_TRAIN_PATH = os.path.join(DATA_DIR, \"interactions_train.csv\") # Needed for counts\n",
    "\n",
    "# Model file paths\n",
    "LGBM_MODEL_PATH = os.path.join(MODEL_DIR, 'lgbm_ranker_model.joblib')\n",
    "LGBM_FEATURES_PATH = os.path.join(MODEL_DIR, 'lgbm_feature_cols.pkl')\n",
    "USER_ENCODER_PATH = os.path.join(MODEL_DIR, 'user_encoder.pkl')\n",
    "ITEM_ENCODER_PATH = os.path.join(MODEL_DIR, 'item_encoder.pkl')\n",
    "USER_EMBEDDINGS_PATH = os.path.join(MODEL_DIR, 'user_embeddings.npy')\n",
    "ITEM_EMBEDDINGS_PATH = os.path.join(MODEL_DIR, 'item_embeddings.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-md-load-models",
   "metadata": {},
   "source": [
    "## Load Trained Models and Supporting Data\n",
    "Load the ranker, feature list, encoders, embeddings, test pairs, and ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-code-load-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading models and supporting data...\")\n",
    "try:\n",
    "    lgbm_ranker = joblib.load(LGBM_MODEL_PATH)\n",
    "    with open(LGBM_FEATURES_PATH, 'rb') as f:\n",
    "        feature_cols = pickle.load(f)\n",
    "    print(f\"Loaded LightGBM model and {len(feature_cols)} feature columns.\")\n",
    "\n",
    "    with open(USER_ENCODER_PATH, 'rb') as f:\n",
    "        user_encoder = pickle.load(f)\n",
    "    with open(ITEM_ENCODER_PATH, 'rb') as f:\n",
    "        item_encoder = pickle.load(f)\n",
    "    user_embeddings = np.load(USER_EMBEDDINGS_PATH)\n",
    "    item_embeddings = np.load(ITEM_EMBEDDINGS_PATH)\n",
    "    factors = user_embeddings.shape[1]\n",
    "    n_users = len(user_encoder.classes_)\n",
    "    n_items = len(item_encoder.classes_)\n",
    "    print(f\"Loaded encoders (n_users={n_users}, n_items={n_items}) and ALS embeddings (factors={factors}).\")\n",
    "\n",
    "    test_interactions = pd.read_csv(INTERACTIONS_TEST_PATH)\n",
    "    print(f\"Loaded {len(test_interactions)} test user-item pairs.\")\n",
    "\n",
    "    with open(GROUND_TRUTH_PATH, 'rb') as f:\n",
    "        test_user_item_map = pickle.load(f)\n",
    "    print(f\"Loaded ground truth map for {len(test_user_item_map)} users.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading file: {e}\")\n",
    "    print(\"Ensure notebooks 1 and 3 were run successfully and files are in correct locations.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-md-load-features",
   "metadata": {},
   "source": [
    "### Load Feature Data for Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-code-load-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading feature source data...\")\n",
    "try:\n",
    "    user_features = pd.read_csv(USER_FEATURES_PATH)\n",
    "    user_features = user_features.set_index('user_id')\n",
    "    user_cat_cols = ['user_active_degree'] + [f'onehot_feat{i}' for i in range(18)]\n",
    "    user_num_cols = ['is_lowactive_period', 'is_live_streamer', 'is_video_author', \n",
    "                     'follow_user_num', 'fans_user_num', 'friend_user_num', 'register_days']\n",
    "    user_feature_cols = user_cat_cols + user_num_cols\n",
    "    # Keep only necessary columns to reduce memory usage\n",
    "    user_features = user_features[[col for col in user_feature_cols if col in user_features.columns]]\n",
    "    print(\"Loaded user features.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"User features file not found. Features relying on it will be imputed.\")\n",
    "    user_features = None\n",
    "    user_cat_cols, user_num_cols = [], []\n",
    "\n",
    "item_metadata = pd.read_csv(VIDEO_METADATA_PATH)\n",
    "def parse_list_string(s):\n",
    "    try: return ast.literal_eval(s) if isinstance(s, str) else []\n",
    "    except: return []\n",
    "item_metadata['feat_list'] = item_metadata['feat'].apply(parse_list_string)\n",
    "item_metadata['num_categories'] = item_metadata['feat_list'].apply(len)\n",
    "item_metadata = item_metadata.set_index('item_id')[['num_categories']]\n",
    "print(\"Loaded and processed item metadata.\")\n",
    "\n",
    "# Calculate interaction counts from the *training* data\n",
    "print(\"Calculating interaction counts from training data...\")\n",
    "train_interactions_full = pd.read_csv(INTERACTIONS_TRAIN_PATH)\n",
    "user_counts = train_interactions_full.groupby('user_id').size().rename('user_interaction_count')\n",
    "item_counts = train_interactions_full.groupby('item_id').size().rename('item_interaction_count')\n",
    "del train_interactions_full # Free memory\n",
    "print(\"Calculated interaction counts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-md-prep-test",
   "metadata": {},
   "source": [
    "## Prepare Features for Test Set\n",
    "Apply the *exact same* feature engineering steps used for the training data in Notebook 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-code-prep-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preparing features for the test set...\")\n",
    "test_data_for_ranker = test_interactions.copy()\n",
    "\n",
    "# --- 1. Merge Features ---\n",
    "if user_features is not None:\n",
    "    test_data_for_ranker = test_data_for_ranker.merge(user_features, on='user_id', how='left')\n",
    "test_data_for_ranker = test_data_for_ranker.merge(item_metadata, on='item_id', how='left')\n",
    "test_data_for_ranker = test_data_for_ranker.merge(user_counts, on='user_id', how='left')\n",
    "test_data_for_ranker = test_data_for_ranker.merge(item_counts, on='item_id', how='left')\n",
    "\n",
    "# --- 2. Add Embeddings --- \n",
    "# Map test user/item IDs to the internal indices learned during training\n",
    "# Handle IDs present in test but not seen during training (assign default index like -1 or handle later during merge)\n",
    "user_map = {id_: idx for idx, id_ in enumerate(user_encoder.classes_)}\n",
    "item_map = {id_: idx for idx, id_ in enumerate(item_encoder.classes_)}\n",
    "test_data_for_ranker['user_idx'] = test_data_for_ranker['user_id'].map(user_map).fillna(-1).astype(int)\n",
    "test_data_for_ranker['item_idx'] = test_data_for_ranker['item_id'].map(item_map).fillna(-1).astype(int)\n",
    "\n",
    "# Create embedding DFs with RangeIndex\n",
    "user_emb_df = pd.DataFrame(user_embeddings, index=pd.RangeIndex(n_users))\n",
    "item_emb_df = pd.DataFrame(item_embeddings, index=pd.RangeIndex(n_items))\n",
    "user_emb_df.columns = [f'user_emb_{i}' for i in range(factors)]\n",
    "item_emb_df.columns = [f'item_emb_{i}' for i in range(factors)]\n",
    "\n",
    "# Merge using internal indices\n",
    "test_data_for_ranker = test_data_for_ranker.merge(user_emb_df, left_on='user_idx', right_index=True, how='left')\n",
    "test_data_for_ranker = test_data_for_ranker.merge(item_emb_df, left_on='item_idx', right_index=True, how='left')\n",
    "\n",
    "# --- 3. Impute Missing Values --- \n",
    "# Impute numerical columns\n",
    "num_cols_to_impute = ['num_categories', 'user_interaction_count', 'item_interaction_count'] + user_num_cols\n",
    "for col in num_cols_to_impute:\n",
    "    if col in test_data_for_ranker.columns:\n",
    "        test_data_for_ranker[col].fillna(0, inplace=True)\n",
    "\n",
    "# Impute categorical user features\n",
    "if user_features is not None:\n",
    "    for col in user_cat_cols:\n",
    "        if col in test_data_for_ranker.columns:\n",
    "            test_data_for_ranker[col].fillna(-1, inplace=True)\n",
    "\n",
    "# Impute embeddings (for users/items not seen in training)\n",
    "emb_cols = [f'user_emb_{i}' for i in range(factors)] + [f'item_emb_{i}' for i in range(factors)]\n",
    "test_data_for_ranker[emb_cols] = test_data_for_ranker[emb_cols].fillna(0)\n",
    "\n",
    "print(f\"NaN count after test imputation: {test_data_for_ranker.isnull().sum().sum()}\")\n",
    "\n",
    "# --- 4. Select Features and Convert Types --- \n",
    "# Ensure feature columns match the order used during training\n",
    "if not all(f in test_data_for_ranker.columns for f in feature_cols):\n",
    "    print(\"ERROR: Test data missing required feature columns!\")\n",
    "    print(f\"Missing: {[f for f in feature_cols if f not in test_data_for_ranker.columns]}\")\n",
    "    exit()\n",
    "X_test = test_data_for_ranker[feature_cols]\n",
    "\n",
    "# Convert categorical features to category type (matching training)\n",
    "if user_features is not None:\n",
    "    for col in user_cat_cols:\n",
    "        if col in X_test.columns:\n",
    "             # Ensure consistency with training categories if possible, \n",
    "             # though LightGBM handles unseen values if dtype is category.\n",
    "             X_test[col] = X_test[col].astype('category')\n",
    "\n",
    "print(f\"Test data feature preparation complete. Shape for prediction: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-md-predict",
   "metadata": {},
   "source": [
    "## Generate Predictions\n",
    "Use the trained LightGBM model to predict the probability of positive interaction for each test pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-code-predict",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating predictions using LightGBM ranker...\")\n",
    "# Predict probability for the positive class (class 1)\n",
    "predictions = lgbm_ranker.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Create dataframe with user, item, and score\n",
    "results_df = test_data_for_ranker[['user_id', 'item_id']].copy()\n",
    "results_df['score'] = predictions\n",
    "\n",
    "print(f\"Generated {len(results_df)} predictions.\")\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-md-evaluate",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance\n",
    "Calculate ranking metrics (Precision@K, Recall@K, NDCG@K) by comparing predicted rankings against the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-code-evaluate",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not test_user_item_map:\n",
    "    print(\"Skipping evaluation as ground truth map is not available.\")\n",
    "else:\n",
    "    print(\"Evaluating model ranking performance...\")\n",
    "    \n",
    "    # Group predictions by user and sort by score to get ranked lists\n",
    "    user_predictions_grouped = results_df.sort_values('score', ascending=False).groupby('user_id')\n",
    "\n",
    "    K_values = [10, 20, 50]\n",
    "    precisions = {k: [] for k in K_values}\n",
    "    recalls = {k: [] for k in K_values}\n",
    "    ndcgs = {k: [] for k in K_values}\n",
    "\n",
    "    evaluated_users_count = 0\n",
    "    # Iterate through users in the ground truth map\n",
    "    for user_id, true_items in tqdm(test_user_item_map.items(), desc=\"Evaluating Users\"):\n",
    "        # Check if user has predictions (they should if test data filtering was consistent)\n",
    "        if user_id not in user_predictions_grouped.groups:\n",
    "            # This user was in test ground truth but has no items in interactions_test.csv\n",
    "            # Or was filtered out before prediction? This shouldn't happen with current logic.\n",
    "            continue \n",
    "\n",
    "        # Get this user's predicted items, ranked by score\n",
    "        user_preds_df = user_predictions_grouped.get_group(user_id)\n",
    "        recommended_items = user_preds_df['item_id'].tolist()\n",
    "\n",
    "        # If true_items is empty for a user (shouldn't happen based on prep), skip\n",
    "        if not true_items:\n",
    "            continue\n",
    "\n",
    "        # Calculate metrics for different K\n",
    "        for k in K_values:\n",
    "            prec = precision_at_k(true_items, recommended_items, k)\n",
    "            rec = recall_at_k(true_items, recommended_items, k)\n",
    "            ndcg = ndcg_at_k(true_items, recommended_items, k)\n",
    "            precisions[k].append(prec)\n",
    "            recalls[k].append(rec)\n",
    "            ndcgs[k].append(ndcg)\n",
    "        \n",
    "        evaluated_users_count += 1\n",
    "\n",
    "    # --- Print Average Metrics --- \n",
    "    print(f\"\\n--- Evaluation Results (on {evaluated_users_count} users with ground truth) ---\")\n",
    "    if evaluated_users_count > 0:\n",
    "        for k in K_values:\n",
    "            avg_precision = np.mean(precisions[k])\n",
    "            avg_recall = np.mean(recalls[k])\n",
    "            avg_ndcg = np.mean(ndcgs[k])\n",
    "            print(f\"@{k}:\")\n",
    "            print(f\"  Avg Precision@{k}: {avg_precision:.4f}\")\n",
    "            print(f\"  Avg Recall@{k}   : {avg_recall:.4f}\")\n",
    "            print(f\"  Avg NDCG@{k}     : {avg_ndcg:.4f}\")\n",
    "    else:\n",
    "        print(\"No users could be evaluated. Check ground truth data and filtering steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-md-submit",
   "metadata": {},
   "source": [
    "## Generate Final Submission File\n",
    "Save the predictions in the required `user_id,item_id,score` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-code-submit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The results_df already has the required columns\n",
    "submission_df = results_df[['user_id', 'item_id', 'score']]\n",
    "\n",
    "print(f\"Saving final submission file with {len(submission_df)} predictions to: {SUBMISSION_PATH}\")\n",
    "submission_df.to_csv(SUBMISSION_PATH, index=False)\n",
    "print(\"--- Submission file created successfully. --- \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (implicit-env)",
   "language": "python",
   "name": "implicit-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
