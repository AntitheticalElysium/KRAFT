{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cdf4a4b",
   "metadata": {},
   "source": [
    "# KRAFT: Model Training\n",
    "\n",
    "This notebook focuses on training the two main components of our recommender system:\n",
    "1.  **ALS (Alternating Least Squares) Model:** For candidate generation. This model is trained on the user-item interaction matrix derived from `big_matrix`.\n",
    "2.  **LightGBM Model:** For ranking the candidates generated by ALS (or for direct ranking if evaluating on `small_matrix`). This model is trained on the feature-rich dataset derived from `big_matrix`.\n",
    "\n",
    "The trained models will be saved to disk for later use in evaluation and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d6c18e",
   "metadata": {},
   "source": [
    "## 1. Imports and Configuration\n",
    "\n",
    "Import necessary libraries and define paths for loading processed data and saving trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7efefe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import json\n",
    "from scipy.sparse import csr_matrix, load_npz\n",
    "import implicit\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "\n",
    "# --- Path Definitions ---\n",
    "RAW_DATA_BASE_PATH = \"../raw_data/KuaiRec/data/\"\n",
    "PROCESSED_DATA_PATH = \"../data/\"\n",
    "MODELS_PATH = \"../models/\"\n",
    "os.makedirs(MODELS_PATH, exist_ok=True)\n",
    "\n",
    "# --- Global Variables\n",
    "TARGET_COL = 'watch_ratio' # Consistent with data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df69b276",
   "metadata": {},
   "source": [
    "## 2. ALS Model Training (Candidate Generation)\n",
    "\n",
    "The ALS model is trained on the sparse user-item interaction matrix created from `big_matrix` interactions. `watch_ratio` is used as the confidence score. \n",
    "\n",
    "**Note:** The `interaction_matrix_als` and ID mappings (`user_to_idx`, `video_to_idx`) are assumed to be created and available from the data preparation phase. For this notebook, if they are not in memory, we would typically load them. However, since this notebook follows the data prep one, we'll proceed assuming they might be in memory or we'd load the necessary components if this were a standalone script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e82f35a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training ALS Model ---\n",
      "Re-creating ALS interaction matrix from processed big_matrix data...\n",
      "Loading minimal big_matrix_interactions for ALS matrix construction...\n",
      "Successfully re-created ALS Sparse Matrix Shape: (7176, 10728), NNZ: 10300969\n",
      "Fitting ALS model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51ad7b70376f451e8e87179d392c3cae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS model training complete.\n",
      "ALS model saved to: ../models/als_model.joblib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"--- Training ALS Model ---\")\n",
    "\n",
    "# Load ALS components (ID mappings and interaction matrix)\n",
    "print(\"Re-creating ALS interaction matrix from processed big_matrix data...\")\n",
    "try:\n",
    "    # Attempt to load df_big_merged components\n",
    "    with open(os.path.join(PROCESSED_DATA_PATH, 'user_to_idx_als.json'), 'r') as f:\n",
    "        user_to_idx = {int(k): v for k, v in json.load(f).items()}\n",
    "    with open(os.path.join(PROCESSED_DATA_PATH, 'video_to_idx_als.json'), 'r') as f:\n",
    "        video_to_idx = {int(k): v for k, v in json.load(f).items()}\n",
    "    \n",
    "    # We need the raw interactions from big_matrix to build the sparse matrix again\n",
    "    print(\"Loading minimal big_matrix_interactions for ALS matrix construction...\")\n",
    "    interaction_cols_initial_load_als = {'user_id': 'float32', 'video_id': 'float32', 'watch_ratio': 'float32'}\n",
    "    interaction_cols_final_dtypes_als = {'user_id': 'int32', 'video_id': 'int32', 'watch_ratio': 'float32'}\n",
    "    \n",
    "    temp_big_interactions = pd.read_csv(os.path.join(RAW_DATA_BASE_PATH, \"big_matrix.csv\"),\n",
    "                                  usecols=interaction_cols_initial_load_als.keys(),\n",
    "                                  dtype=interaction_cols_initial_load_als)\n",
    "    # Simplified post_process for this temp load\n",
    "    for col in ['user_id', 'video_id']:\n",
    "        temp_big_interactions[col] = temp_big_interactions[col].fillna(-1).astype(interaction_cols_final_dtypes_als[col])\n",
    "    temp_big_interactions['watch_ratio'] = temp_big_interactions['watch_ratio'].astype(interaction_cols_final_dtypes_als['watch_ratio'])\n",
    "\n",
    "    # Filter out any interactions where user_id or video_id is not in our mappings\n",
    "    temp_big_interactions = temp_big_interactions[\n",
    "        temp_big_interactions['user_id'].isin(user_to_idx.keys()) &\n",
    "        temp_big_interactions['video_id'].isin(video_to_idx.keys())\n",
    "    ]\n",
    "\n",
    "    als_user_ids = temp_big_interactions['user_id'].map(user_to_idx)\n",
    "    als_item_ids = temp_big_interactions['video_id'].map(video_to_idx)\n",
    "    als_ratings = temp_big_interactions['watch_ratio']\n",
    "    als_ratings_clipped = np.maximum(als_ratings, 0.001)\n",
    "    \n",
    "    num_users_als = len(user_to_idx)\n",
    "    num_videos_als = len(video_to_idx)\n",
    "\n",
    "    interaction_matrix_als = csr_matrix((als_ratings_clipped, (als_user_ids, als_item_ids)),\n",
    "                                        shape=(num_users_als, num_videos_als))\n",
    "    print(f\"Successfully re-created ALS Sparse Matrix Shape: {interaction_matrix_als.shape}, NNZ: {interaction_matrix_als.nnz}\")\n",
    "    del temp_big_interactions, als_user_ids, als_item_ids, als_ratings, als_ratings_clipped\n",
    "    gc.collect()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: ALS ID mapping files not found in processed_data. Please run Data Preparation notebook first.\")\n",
    "    raise\n",
    "\n",
    "# ALS Model Configuration\n",
    "als_params = {\n",
    "    'factors': 100, \n",
    "    'regularization': 0.1,\n",
    "    'iterations': 20,\n",
    "    'use_cg': True,\n",
    "    'calculate_training_loss': True,\n",
    "    'random_state': 42 # For reproducibility\n",
    "}\n",
    "\n",
    "als_model = implicit.als.AlternatingLeastSquares(**als_params)\n",
    "\n",
    "# Train the ALS model (expects user-item matrix)\n",
    "print(\"Fitting ALS model...\")\n",
    "als_model.fit(interaction_matrix_als)\n",
    "print(\"ALS model training complete.\")\n",
    "\n",
    "# Save the trained ALS model\n",
    "als_model_path = os.path.join(MODELS_PATH, \"als_model.joblib\")\n",
    "joblib.dump(als_model, als_model_path)\n",
    "print(f\"ALS model saved to: {als_model_path}\")\n",
    "\n",
    "del interaction_matrix_als, als_model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a2ccbe",
   "metadata": {},
   "source": [
    "## 3. LightGBM Model Training (Ranking)\n",
    "\n",
    "The LightGBM model is trained for the ranking task using the feature-engineered training data derived from `big_matrix`. It predicts the `watch_ratio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44c124d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training LightGBM Model ---\n",
      "Loading LightGBM training data...\n",
      "Loaded LightGBM training data: (10024644, 42)\n",
      "Verifying and casting dtypes for categorical features in training data...\n",
      "Identified 29 categorical features for LGBM training.\n",
      "Creating LightGBM training dataset...\n",
      "Training LightGBM model for 1000 rounds...\n",
      "LightGBM model training complete.\n",
      "LightGBM model saved to: ../models/lightgbm_ranker_model.txt\n",
      "\n",
      "--- Model Training Phase Complete. Models are saved in ../models/ ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Training LightGBM Model ---\")\n",
    "\n",
    "# Load preprocessed training data for LightGBM\n",
    "print(\"Loading LightGBM training data...\")\n",
    "train_lgbm_parquet_path = os.path.join(PROCESSED_DATA_PATH, 'lightgbm_train_data.parquet')\n",
    "try:\n",
    "    train_lgbm_df = pd.read_parquet(train_lgbm_parquet_path)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {train_lgbm_parquet_path} not found. Please run Data Preparation notebook first.\")\n",
    "    raise\n",
    "\n",
    "print(f\"Loaded LightGBM training data: {train_lgbm_df.shape}\")\n",
    "\n",
    "y_train_lgbm = train_lgbm_df[TARGET_COL]\n",
    "X_train_lgbm = train_lgbm_df.drop(columns=[TARGET_COL])\n",
    "del train_lgbm_df\n",
    "gc.collect()\n",
    "\n",
    "# Determine categorical features for LightGBM\n",
    "base_categorical_features = ['user_id', 'video_id', 'user_active_degree', \n",
    "                             'interaction_hour', 'interaction_day_of_week']\n",
    "user_flag_categoricals = ['is_lowactive_period', 'is_live_streamer', 'is_video_author']\n",
    "onehot_feature_names_train = [f'onehot_feat{i}' for i in range(18)]\n",
    "daily_item_categoricals = ['author_id', 'video_type', 'video_tag_id']\n",
    "\n",
    "categorical_features_for_lgbm_training = []\n",
    "for col_list in [base_categorical_features, user_flag_categoricals, onehot_feature_names_train, daily_item_categoricals]:\n",
    "    for col in col_list:\n",
    "        if col in X_train_lgbm.columns:\n",
    "            categorical_features_for_lgbm_training.append(col)\n",
    "\n",
    "# Ensure categorical features have 'category' dtype\n",
    "print(\"Verifying and casting dtypes for categorical features in training data...\")\n",
    "for col in categorical_features_for_lgbm_training:\n",
    "    if X_train_lgbm[col].dtype.name != 'category':\n",
    "        X_train_lgbm[col] = X_train_lgbm[col].astype('category')\n",
    "print(f\"Identified {len(categorical_features_for_lgbm_training)} categorical features for LGBM training.\")\n",
    "\n",
    "# LightGBM Model Configuration\n",
    "lgbm_train_params = {\n",
    "    'objective': 'regression_l1', # MAE is often robust for watch ratio like targets\n",
    "    'metric': ['mae', 'rmse'], # Metrics to monitor if a validation set were used\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 63,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,    # Column subsampling\n",
    "    'bagging_fraction': 0.8,    # Row subsampling\n",
    "    'bagging_freq': 1,          # Perform bagging at every iteration\n",
    "    'verbose': -1,              \n",
    "    'n_jobs': -1,               \n",
    "    'seed': 42                  # For reproducibility\n",
    "}\n",
    "num_boost_round_lgbm = 1000 # Number of boosting iterations\n",
    "\n",
    "print(\"Creating LightGBM training dataset...\")\n",
    "lgb_train_dataset = lgb.Dataset(X_train_lgbm, y_train_lgbm,\n",
    "                                categorical_feature=categorical_features_for_lgbm_training,\n",
    "                                free_raw_data=False) # Keep raw data for feature importance\n",
    "del X_train_lgbm, y_train_lgbm\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Training LightGBM model for {num_boost_round_lgbm} rounds...\")\n",
    "model_lgbm_trained = lgb.train(\n",
    "    params=lgbm_train_params,\n",
    "    train_set=lgb_train_dataset,\n",
    "    num_boost_round=num_boost_round_lgbm\n",
    "    # Early stopping, add valid_sets and callbacks=[lgb.early_stopping(...)]?\n",
    ")\n",
    "print(\"LightGBM model training complete.\")\n",
    "\n",
    "# Save the trained LightGBM model\n",
    "lgbm_model_path = os.path.join(MODELS_PATH, \"lightgbm_ranker_model.txt\")\n",
    "model_lgbm_trained.save_model(lgbm_model_path)\n",
    "print(f\"LightGBM model saved to: {lgbm_model_path}\")\n",
    "\n",
    "del lgb_train_dataset, model_lgbm_trained\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\n--- Model Training Phase Complete. Models are saved in ../models/ ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a289877-696c-4784-83af-55b0b2ff8d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (implicit-env)",
   "language": "python",
   "name": "implicit-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
