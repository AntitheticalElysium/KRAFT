{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "train-md-intro",
   "metadata": {},
   "source": [
    "# 3. Model Training\n",
    "\n",
    "**Goal:** Train the recommendation models based on the multi-stage hybrid approach:\n",
    "1.  **Stage 1: Candidate Generation Model:** Alternating Least Squares (ALS) for collaborative filtering to learn user and item embeddings.\n",
    "2.  **Stage 2: Ranking Model:** LightGBM using rich features (user profile, item metadata, interaction counts, and ALS embeddings) to predict the likelihood of a positive interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-md-setup",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Import libraries, define paths, and create necessary directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "train-code-imports-paths",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "import implicit # Library for ALS\n",
    "import lightgbm as lgb # Library for Ranking Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib # For saving scikit-learn style models\n",
    "import pickle # For saving Python objects (like encoders, feature lists)\n",
    "import os\n",
    "import ast # For parsing list strings from CSV\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_DIR = \"../data/\"\n",
    "RAW_DATA_DIR = \"../raw_data/KuaiRec/data/\"\n",
    "MODEL_DIR = \"../models/\"\n",
    "\n",
    "# Input file paths\n",
    "INTERACTIONS_TRAIN_PATH = os.path.join(DATA_DIR, \"interactions_train.csv\")\n",
    "USER_FEATURES_PATH = os.path.join(RAW_DATA_DIR, \"user_features.csv\")\n",
    "VIDEO_METADATA_PATH = os.path.join(DATA_DIR, \"video_metadata.csv\")\n",
    "\n",
    "# Output file paths\n",
    "USER_ENCODER_PATH = os.path.join(MODEL_DIR, 'user_encoder.pkl')\n",
    "ITEM_ENCODER_PATH = os.path.join(MODEL_DIR, 'item_encoder.pkl')\n",
    "ALS_MODEL_PATH = os.path.join(MODEL_DIR, 'als_model.joblib')\n",
    "USER_EMBEDDINGS_PATH = os.path.join(MODEL_DIR, 'user_embeddings.npy')\n",
    "ITEM_EMBEDDINGS_PATH = os.path.join(MODEL_DIR, 'item_embeddings.npy')\n",
    "LGBM_MODEL_PATH = os.path.join(MODEL_DIR, 'lgbm_ranker_model.joblib')\n",
    "LGBM_FEATURES_PATH = os.path.join(MODEL_DIR, 'lgbm_feature_cols.pkl')\n",
    "\n",
    "# Create model directory if it doesn't exist\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-md-als",
   "metadata": {},
   "source": [
    "## Stage 1: Candidate Generation Model (ALS)\n",
    "Train ALS on user-item interactions to generate embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-md-als-load",
   "metadata": {},
   "source": [
    "### Load and Prepare Interaction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "train-code-als-load-prep",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training interactions...\n",
      "Loaded 3741256 training interactions.\n",
      "Number of unique users (mapped): 1411\n",
      "Number of unique items (mapped): 2993\n",
      "Saved user encoder to ../models/user_encoder.pkl\n",
      "Saved item encoder to ../models/item_encoder.pkl\n",
      "Creating sparse interaction matrix (users x items)...\n",
      "Created sparse interaction matrix with shape: (1411, 2993)\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading training interactions...\")\n",
    "train_interactions = pd.read_csv(INTERACTIONS_TRAIN_PATH)\n",
    "print(f\"Loaded {len(train_interactions)} training interactions.\")\n",
    "\n",
    "# Create label encoders for mapping original IDs to contiguous 0-based indices\n",
    "user_encoder = LabelEncoder()\n",
    "item_encoder = LabelEncoder()\n",
    "\n",
    "train_interactions['user_idx'] = user_encoder.fit_transform(train_interactions['user_id'])\n",
    "train_interactions['item_idx'] = item_encoder.fit_transform(train_interactions['item_id'])\n",
    "\n",
    "n_users = len(user_encoder.classes_)\n",
    "n_items = len(item_encoder.classes_)\n",
    "print(f\"Number of unique users (mapped): {n_users}\")\n",
    "print(f\"Number of unique items (mapped): {n_items}\")\n",
    "\n",
    "# Save the fitted encoders for later use (e.g., in prediction/evaluation)\n",
    "with open(USER_ENCODER_PATH, 'wb') as f:\n",
    "    pickle.dump(user_encoder, f)\n",
    "with open(ITEM_ENCODER_PATH, 'wb') as f:\n",
    "    pickle.dump(item_encoder, f)\n",
    "print(f\"Saved user encoder to {USER_ENCODER_PATH}\")\n",
    "print(f\"Saved item encoder to {ITEM_ENCODER_PATH}\")\n",
    "\n",
    "# Create sparse interaction matrix (users x items) for implicit library\n",
    "# Using 'positive_interaction' (1 for positive, 0 otherwise) as implicit feedback strength.\n",
    "print(\"Creating sparse interaction matrix (users x items)...\")\n",
    "sparse_interaction_matrix_user_item = sparse.csr_matrix((\n",
    "    train_interactions['positive_interaction'],\n",
    "    (train_interactions['user_idx'], train_interactions['item_idx'])\n",
    "), shape=(n_users, n_items))\n",
    "\n",
    "print(f\"Created sparse interaction matrix with shape: {sparse_interaction_matrix_user_item.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-md-als-train",
   "metadata": {},
   "source": [
    "### Train ALS Model\n",
    "Using the `implicit` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "train-code-als-train",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ALS model (factors=64, regularization=0.01, iterations=20)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ac672233c04dd6a644949a75b16f6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS model training complete.\n"
     ]
    }
   ],
   "source": [
    "# ALS Hyperparameters\n",
    "factors = 64        # Embedding dimensionality\n",
    "regularization = 0.01 # L2 regularization\n",
    "iterations = 20     # Number of optimization iterations\n",
    "\n",
    "print(f\"Training ALS model (factors={factors}, regularization={regularization}, iterations={iterations})...\")\n",
    "als_model = implicit.als.AlternatingLeastSquares(\n",
    "    factors=factors,\n",
    "    regularization=regularization,\n",
    "    iterations=iterations,\n",
    "    calculate_training_loss=True,\n",
    "    random_state=42,\n",
    "    use_gpu=implicit.gpu.HAS_CUDA # Automatically use GPU if available\n",
    ")\n",
    "\n",
    "# Train the model on the user-item matrix\n",
    "als_model.fit(sparse_interaction_matrix_user_item)\n",
    "\n",
    "print(\"ALS model training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-md-als-save",
   "metadata": {},
   "source": [
    "### Save ALS Model and Embeddings\n",
    "Save the trained model object and the learned user/item embeddings (factors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "train-code-als-save",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ALS model object to ../models/als_model.joblib\n",
      "Converted factors from GPU/Implicit format using .to_numpy()\n",
      "Saved user embeddings to ../models/user_embeddings.npy (shape: (1411, 64))\n",
      "Saved item embeddings to ../models/item_embeddings.npy (shape: (2993, 64))\n"
     ]
    }
   ],
   "source": [
    "# Save the ALS model object\n",
    "joblib.dump(als_model, ALS_MODEL_PATH)\n",
    "print(f\"Saved ALS model object to {ALS_MODEL_PATH}\")\n",
    "\n",
    "# Extract embeddings, converting from GPU if necessary\n",
    "try:\n",
    "    user_factors_np = als_model.user_factors.to_numpy()\n",
    "    item_factors_np = als_model.item_factors.to_numpy()\n",
    "    print(\"Converted factors from GPU/Implicit format using .to_numpy()\")\n",
    "except AttributeError:\n",
    "    print(\"Factors don't have .to_numpy(), attempting direct conversion/use.\")\n",
    "    user_factors_np = np.array(als_model.user_factors)\n",
    "    item_factors_np = np.array(als_model.item_factors)\n",
    "\n",
    "# Verify shapes before saving\n",
    "if user_factors_np.shape != (n_users, factors) or item_factors_np.shape != (n_items, factors):\n",
    "    print(f\"ERROR: Unexpected embedding shapes! User: {user_factors_np.shape} (Expected {(n_users, factors)}), Item: {item_factors_np.shape} (Expected {(n_items, factors)})\")\n",
    "else:\n",
    "    # Save the NumPy arrays\n",
    "    np.save(USER_EMBEDDINGS_PATH, user_factors_np)\n",
    "    np.save(ITEM_EMBEDDINGS_PATH, item_factors_np)\n",
    "    print(f\"Saved user embeddings to {USER_EMBEDDINGS_PATH} (shape: {user_factors_np.shape})\")\n",
    "    print(f\"Saved item embeddings to {ITEM_EMBEDDINGS_PATH} (shape: {item_factors_np.shape})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-md-lgbm",
   "metadata": {},
   "source": [
    "## Stage 2: Ranking Model (LightGBM)\n",
    "Prepare features and train the LightGBM classifier to predict `positive_interaction`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-md-lgbm-features",
   "metadata": {},
   "source": [
    "### Feature Engineering for Ranker\n",
    "Combine various features into a single DataFrame suitable for LightGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "train-code-lgbm-features",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading feature data for ranker...\n",
      "Loaded user features for 7176 users.\n",
      "Loaded item metadata for 10728 items.\n",
      "Loaded ALS embeddings with 64 factors.\n",
      "Creating base training data for ranker...\n",
      "Merging features...\n",
      "Finished merging features.\n",
      "NaN count before imputation: 284158\n",
      "NaN count after imputation: 0\n",
      "\n",
      "Created ranker training data with 157 features.\n",
      "Shape: (3741256, 162)\n",
      "Categorical features for LightGBM: ['user_active_degree', 'onehot_feat0', 'onehot_feat1', 'onehot_feat2', 'onehot_feat3', 'onehot_feat4', 'onehot_feat5', 'onehot_feat6', 'onehot_feat7', 'onehot_feat8', 'onehot_feat9', 'onehot_feat10', 'onehot_feat11', 'onehot_feat12', 'onehot_feat13', 'onehot_feat14', 'onehot_feat15', 'onehot_feat16', 'onehot_feat17']\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load necessary raw/processed feature data ---\n",
    "print(\"Loading feature data for ranker...\")\n",
    "try:\n",
    "    user_features = pd.read_csv(USER_FEATURES_PATH)\n",
    "    user_features = user_features.set_index('user_id')\n",
    "    # Define user feature columns to use\n",
    "    user_cat_cols = ['user_active_degree'] + [f'onehot_feat{i}' for i in range(18)]\n",
    "    user_num_cols = ['is_lowactive_period', 'is_live_streamer', 'is_video_author', \n",
    "                     'follow_user_num', 'fans_user_num', 'friend_user_num', 'register_days']\n",
    "    user_feature_cols = user_cat_cols + user_num_cols\n",
    "    user_features = user_features[user_feature_cols]\n",
    "    print(f\"Loaded user features for {len(user_features)} users.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"User features file not found. Proceeding without them.\")\n",
    "    user_features = None\n",
    "    user_cat_cols, user_num_cols, user_feature_cols = [], [], []\n",
    "\n",
    "item_metadata = pd.read_csv(VIDEO_METADATA_PATH)\n",
    "def parse_list_string(s):\n",
    "    try: return ast.literal_eval(s) if isinstance(s, str) else []\n",
    "    except: return []\n",
    "item_metadata['feat_list'] = item_metadata['feat'].apply(parse_list_string)\n",
    "item_metadata['num_categories'] = item_metadata['feat_list'].apply(len) # Example item feature\n",
    "item_metadata = item_metadata.set_index('item_id')[['num_categories']] \n",
    "print(f\"Loaded item metadata for {len(item_metadata)} items.\")\n",
    "\n",
    "# Load ALS embeddings (already saved as NumPy arrays)\n",
    "try:\n",
    "    user_embeddings = np.load(USER_EMBEDDINGS_PATH)\n",
    "    item_embeddings = np.load(ITEM_EMBEDDINGS_PATH)\n",
    "    factors = user_embeddings.shape[1]\n",
    "    print(f\"Loaded ALS embeddings with {factors} factors.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: ALS embedding files not found. Cannot proceed with ranker training.\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Create base DataFrame with interactions and target ---\n",
    "print(\"Creating base training data for ranker...\")\n",
    "# Use interactions from train_interactions which already has user_idx, item_idx\n",
    "ranker_train_df = train_interactions[['user_id', 'item_id', 'user_idx', 'item_idx', 'positive_interaction']].copy()\n",
    "\n",
    "# --- 3. Merge features onto the base DataFrame ---\n",
    "print(\"Merging features...\")\n",
    "# User Features\n",
    "if user_features is not None:\n",
    "    ranker_train_df = ranker_train_df.merge(user_features, on='user_id', how='left')\n",
    "\n",
    "# Item Features\n",
    "ranker_train_df = ranker_train_df.merge(item_metadata, on='item_id', how='left')\n",
    "\n",
    "# ALS Embeddings (using user_idx/item_idx)\n",
    "user_emb_df = pd.DataFrame(user_embeddings, index=pd.RangeIndex(n_users))\n",
    "item_emb_df = pd.DataFrame(item_embeddings, index=pd.RangeIndex(n_items))\n",
    "user_emb_df.columns = [f'user_emb_{i}' for i in range(factors)]\n",
    "item_emb_df.columns = [f'item_emb_{i}' for i in range(factors)]\n",
    "ranker_train_df = ranker_train_df.merge(user_emb_df, left_on='user_idx', right_index=True, how='left')\n",
    "ranker_train_df = ranker_train_df.merge(item_emb_df, left_on='item_idx', right_index=True, how='left')\n",
    "\n",
    "# Interaction Count Features (calculated on the fly from the merged df)\n",
    "user_counts = ranker_train_df.groupby('user_id')['item_id'].transform('size')\n",
    "item_counts = ranker_train_df.groupby('item_id')['user_id'].transform('size')\n",
    "ranker_train_df['user_interaction_count'] = user_counts\n",
    "ranker_train_df['item_interaction_count'] = item_counts\n",
    "\n",
    "print(\"Finished merging features.\")\n",
    "\n",
    "# --- 4. Impute Missing Values --- \n",
    "print(f\"NaN count before imputation: {ranker_train_df.isnull().sum().sum()}\")\n",
    "# Impute numeric features (e.g., counts, item metadata) potentially missing due to 'left' merge\n",
    "num_cols_to_impute = ['num_categories', 'user_interaction_count', 'item_interaction_count'] + user_num_cols\n",
    "for col in num_cols_to_impute:\n",
    "    if col in ranker_train_df.columns:\n",
    "        ranker_train_df[col].fillna(0, inplace=True) # Impute numerical with 0\n",
    "\n",
    "# Impute categorical user features with a placeholder (e.g., -1 or 'Missing')\n",
    "if user_features is not None:\n",
    "    for col in user_cat_cols:\n",
    "        if col in ranker_train_df.columns:\n",
    "            ranker_train_df[col].fillna(-1, inplace=True) # Use -1 for missing categories\n",
    "\n",
    "# Impute embeddings (should be rare if data prep is correct)\n",
    "emb_cols = [f'user_emb_{i}' for i in range(factors)] + [f'item_emb_{i}' for i in range(factors)]\n",
    "ranker_train_df[emb_cols] = ranker_train_df[emb_cols].fillna(0)\n",
    "\n",
    "print(f\"NaN count after imputation: {ranker_train_df.isnull().sum().sum()}\")\n",
    "if ranker_train_df.isnull().sum().sum() > 0:\n",
    "    print(\"Warning: NaNs remain after imputation! Check columns:\")\n",
    "    print(ranker_train_df.isnull().sum()[ranker_train_df.isnull().sum() > 0])\n",
    "\n",
    "# --- 5. Define Feature Set and Convert Types ---\n",
    "target = 'positive_interaction'\n",
    "exclude_cols = ['user_id', 'item_id', 'user_idx', 'item_idx', target, 'feat_list'] # Exclude IDs and intermediate lists\n",
    "feature_cols = [col for col in ranker_train_df.columns if col not in exclude_cols]\n",
    "\n",
    "# Convert identified categorical columns to 'category' dtype for LightGBM\n",
    "categorical_features_lgbm = []\n",
    "if user_features is not None:\n",
    "    for col in user_cat_cols:\n",
    "        if col in ranker_train_df.columns:\n",
    "             ranker_train_df[col] = ranker_train_df[col].astype('category')\n",
    "             categorical_features_lgbm.append(col)\n",
    "\n",
    "# Add other known categorical item features if loaded (e.g., 'author_id')\n",
    "# Example: if 'author_id' was loaded and merged:\n",
    "# if 'author_id' in ranker_train_df.columns:\n",
    "#    ranker_train_df['author_id'] = ranker_train_df['author_id'].fillna(-1) # Impute first\n",
    "#    ranker_train_df['author_id'] = ranker_train_df['author_id'].astype('category')\n",
    "#    categorical_features_lgbm.append('author_id')\n",
    "\n",
    "print(f\"\\nCreated ranker training data with {len(feature_cols)} features.\")\n",
    "print(f\"Shape: {ranker_train_df.shape}\")\n",
    "print(f\"Categorical features for LightGBM: {categorical_features_lgbm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-md-lgbm-train-split",
   "metadata": {},
   "source": [
    "### Split Data and Train LightGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "train-code-lgbm-train",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM training set size: 2993004\n",
      "LightGBM validation set size: 748252\n",
      "\n",
      "Training LightGBM ranking model...\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[999]\tvalid_0's auc: 0.852387\n",
      "LightGBM model training complete.\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for LightGBM training\n",
    "X = ranker_train_df[feature_cols]\n",
    "y = ranker_train_df[target]\n",
    "\n",
    "# Split ranker data for training and validation (e.g., 80/20 random split)\n",
    "# Using a validation set helps tune parameters and prevents overfitting via early stopping.\n",
    "X_train_lgbm, X_val_lgbm, y_train_lgbm, y_val_lgbm = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y # Stratify ensures similar target distribution\n",
    ")\n",
    "\n",
    "print(f\"LightGBM training set size: {len(X_train_lgbm)}\")\n",
    "print(f\"LightGBM validation set size: {len(X_val_lgbm)}\")\n",
    "\n",
    "# LightGBM Parameters (Example values - tuning is crucial for performance)\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',         # Area Under ROC Curve - good for ranking tasks\n",
    "    'boosting_type': 'gbdt', # Standard Gradient Boosted Decision Trees\n",
    "    'n_estimators': 1000,    # Max number of trees (use early stopping to find optimal)\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 31,        # Controls complexity of trees\n",
    "    'max_depth': -1,         # No limit on depth\n",
    "    'seed': 42,\n",
    "    'n_jobs': -1,            # Use all available CPU cores\n",
    "    'verbose': -1,           # Suppress verbose training output\n",
    "    'colsample_bytree': 0.8, # Fraction of features used per tree\n",
    "    'subsample': 0.8,        # Fraction of data used per tree (requires boosting_type='gbdt')\n",
    "    'reg_alpha': 0.1,        # L1 regularization\n",
    "    'reg_lambda': 0.1,       # L2 regularization\n",
    "    'max_bin': 128\n",
    "}\n",
    "\n",
    "print(\"\\nTraining LightGBM ranking model...\")\n",
    "lgbm_ranker = lgb.LGBMClassifier(**params)\n",
    "\n",
    "lgbm_ranker.fit(\n",
    "    X_train_lgbm,\n",
    "    y_train_lgbm,\n",
    "    eval_set=[(X_val_lgbm, y_val_lgbm)],\n",
    "    eval_metric='auc',\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=True)], # Stop if validation AUC doesn't improve\n",
    "    categorical_feature=categorical_features_lgbm # Pass names of categorical features\n",
    ")\n",
    "\n",
    "print(\"LightGBM model training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-md-lgbm-save",
   "metadata": {},
   "source": [
    "### Save Trained Ranker Model and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "train-code-lgbm-save",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LightGBM model to ../models/lgbm_ranker_model.joblib\n",
      "Saved LightGBM feature column list to ../models/lgbm_feature_cols.pkl\n",
      "\n",
      "--- Model Training Complete ---\n"
     ]
    }
   ],
   "source": [
    "# Save the trained LightGBM model\n",
    "joblib.dump(lgbm_ranker, LGBM_MODEL_PATH)\n",
    "print(f\"Saved LightGBM model to {LGBM_MODEL_PATH}\")\n",
    "\n",
    "# Save the list of feature columns used by the model\n",
    "# This is crucial for ensuring the same features are used during prediction/evaluation\n",
    "with open(LGBM_FEATURES_PATH, 'wb') as f:\n",
    "    pickle.dump(feature_cols, f)\n",
    "print(f\"Saved LightGBM feature column list to {LGBM_FEATURES_PATH}\")\n",
    "\n",
    "print(\"\\n--- Model Training Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (implicit-env)",
   "language": "python",
   "name": "implicit-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
