{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39dc6876",
   "metadata": {},
   "source": [
    "# KRAFT: Exploratory Data Analysis\n",
    "\n",
    "This notebook performs an exploratory data analysis on the KuaiRec dataset. The goals are to:\n",
    "1. Understand the basic statistics and distributions of users, items, and interactions.\n",
    "2. Analyze the characteristics of user features and item features.\n",
    "3. Investigate the target variable (`watch_ratio`) and its relationship with other features.\n",
    "4. Gain insights that can inform feature engineering, model selection, and evaluation strategies for the recommender system.\n",
    "\n",
    "We will primarily load raw data files for this EDA to inspect their original state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec2a31f",
   "metadata": {},
   "source": [
    "## 1. Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b4679a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# --- Path Definitions ---\n",
    "RAW_DATA_BASE_PATH = \"../raw_data/KuaiRec/data/\"\n",
    "\n",
    "# --- Plotting Configuration ---\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372df0bb",
   "metadata": {},
   "source": [
    "## 2. Loading Data for EDA\n",
    "\n",
    "We will load the key raw data files. For large files like `big_matrix.csv`, we might load a sample or specific columns if full loading is too memory-intensive for interactive EDA, but for this dataset size, full load with selected columns is often feasible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4396786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns and dtypes for leaner loading during EDA\n",
    "interaction_eda_cols = {\n",
    "    'user_id': 'int32', 'video_id': 'int32', \n",
    "    'play_duration': 'int32', 'video_duration': 'int32', \n",
    "    'time': 'str', 'date': 'int32', 'watch_ratio': 'float32'\n",
    "}\n",
    "user_features_eda_cols = { # Load most user features for EDA\n",
    "    'user_id': 'int32', 'user_active_degree': 'category',\n",
    "    'is_lowactive_period': 'int8', 'is_live_streamer': 'int8', \n",
    "    'is_video_author': 'int8', 'follow_user_num': 'int32',\n",
    "    'fans_user_num': 'int32', 'friend_user_num': 'int32', \n",
    "    'register_days': 'int32'\n",
    "    # Skipping onehot_feat* for initial EDA overview, can be added if needed\n",
    "}\n",
    "item_categories_eda_cols = {'video_id': 'int32', 'feat': 'str'}\n",
    "item_daily_eda_cols = { # Select key daily features\n",
    "    'video_id': 'int32', 'date': 'int32', 'author_id': 'int32',\n",
    "    'video_type': 'category', 'upload_dt': 'str', 'video_duration': 'float32',\n",
    "    'show_cnt': 'int32', 'play_cnt': 'int32', 'like_cnt': 'int32',\n",
    "    'play_progress': 'float32'\n",
    "}\n",
    "\n",
    "print(\"Loading big_matrix.csv for EDA...\")\n",
    "try:\n",
    "    df_big = pd.read_csv(os.path.join(RAW_DATA_BASE_PATH, \"big_matrix.csv\"), \n",
    "                         usecols=interaction_eda_cols.keys(), \n",
    "                         dtype=interaction_eda_cols)\n",
    "    print(f\"big_matrix loaded: {df_big.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading big_matrix: {e}\")\n",
    "    df_big = pd.DataFrame() # Empty df if load fails\n",
    "\n",
    "print(\"\\nLoading small_matrix.csv for EDA...\")\n",
    "try:\n",
    "    df_small = pd.read_csv(os.path.join(RAW_DATA_BASE_PATH, \"small_matrix.csv\"), \n",
    "                           usecols=interaction_eda_cols.keys(), \n",
    "                           dtype=interaction_eda_cols)\n",
    "    print(f\"small_matrix loaded: {df_small.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading small_matrix: {e}\")\n",
    "    df_small = pd.DataFrame()\n",
    "\n",
    "print(\"\\nLoading user_features.csv for EDA...\")\n",
    "try:\n",
    "    df_users = pd.read_csv(os.path.join(RAW_DATA_BASE_PATH, \"user_features.csv\"), \n",
    "                           usecols=user_features_eda_cols.keys(),\n",
    "                           dtype=user_features_eda_cols)\n",
    "    print(f\"user_features loaded: {df_users.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading user_features: {e}\")\n",
    "    df_users = pd.DataFrame()\n",
    "\n",
    "print(\"\\nLoading item_categories.csv for EDA...\")\n",
    "try:\n",
    "    df_items_cat = pd.read_csv(os.path.join(RAW_DATA_BASE_PATH, \"item_categories.csv\"), \n",
    "                               usecols=item_categories_eda_cols.keys(),\n",
    "                               dtype=item_categories_eda_cols)\n",
    "    # Process 'feat' to get number of tags\n",
    "    def parse_feat_list(feat_str):\n",
    "        if pd.isna(feat_str) or not isinstance(feat_str, str): return 0\n",
    "        try: return len(eval(feat_str))\n",
    "        except: return 0\n",
    "    df_items_cat['num_tags'] = df_items_cat['feat'].apply(parse_feat_list).astype('int16')\n",
    "    print(f\"item_categories loaded and processed: {df_items_cat.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading item_categories: {e}\")\n",
    "    df_items_cat = pd.DataFrame()\n",
    "\n",
    "print(\"\\nLoading item_daily_features.csv for EDA...\")\n",
    "try:\n",
    "    df_items_daily = pd.read_csv(os.path.join(RAW_DATA_BASE_PATH, \"item_daily_features.csv\"), \n",
    "                                 usecols=item_daily_eda_cols.keys(),\n",
    "                                 dtype=item_daily_eda_cols,\n",
    "                                 # nrows=500000 # Optional: load a sample for quicker EDA on very large daily file\n",
    "                                )\n",
    "    print(f\"item_daily_features loaded: {df_items_daily.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading item_daily_features: {e}\")\n",
    "    df_items_daily = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e8dac6",
   "metadata": {},
   "source": [
    "## 3. Basic Statistics and Sparsity\n",
    "\n",
    "Understanding the scale of the data (users, items, interactions) and the sparsity of the interaction matrices is fundamental."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f099c565",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_big.empty:\n",
    "    num_users_big = df_big['user_id'].nunique()\n",
    "    num_items_big = df_big['video_id'].nunique()\n",
    "    num_interactions_big = len(df_big)\n",
    "    sparsity_big = 1 - (num_interactions_big / (num_users_big * num_items_big))\n",
    "    print(\"--- Big Matrix Statistics ---\")\n",
    "    print(f\"Number of unique users: {num_users_big}\")\n",
    "    print(f\"Number of unique items: {num_items_big}\")\n",
    "    print(f\"Number of interactions: {num_interactions_big}\")\n",
    "    print(f\"Sparsity: {sparsity_big:.4f} ({(sparsity_big * 100):.2f}% empty)\")\n",
    "    \n",
    "    # Dataset description gives density 16.3%, so sparsity is 1 - 0.163 = 0.837.\n",
    "    # This matches if we use the total unique users/items from the full dataset, not just those in big_matrix interactions\n",
    "    total_users_dataset = 7176\n",
    "    total_items_dataset = 10728 \n",
    "    density_big_reported = num_interactions_big / (total_users_dataset * total_items_dataset)\n",
    "    print(f\"Density (based on all users/items in dataset): {density_big_reported:.4f} ({(density_big_reported * 100):.2f}%)\")\n",
    "\n",
    "if not df_small.empty:\n",
    "    num_users_small = df_small['user_id'].nunique()\n",
    "    num_items_small = df_small['video_id'].nunique()\n",
    "    num_interactions_small = len(df_small)\n",
    "    density_small_reported = num_interactions_small / (num_users_small * num_items_small) # Small matrix is dense for its users/items\n",
    "    print(\"\\n--- Small Matrix Statistics ---\")\n",
    "    print(f\"Number of unique users: {num_users_small}\")\n",
    "    print(f\"Number of unique items: {num_items_small}\")\n",
    "    print(f\"Number of interactions: {num_interactions_small}\")\n",
    "    print(f\"Density (for its own scope): {density_small_reported:.4f} ({(density_small_reported * 100):.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad21598",
   "metadata": {},
   "source": [
    "**Justification for Model Choices (Initial Thoughts):**\n",
    "- The high sparsity of `big_matrix` suggests that methods good with sparse data (like Matrix Factorization variants - ALS, or hybrid models - LightGBM with good features) are appropriate.\n",
    "- The presence of rich side information (user and item features) strongly motivates a hybrid approach over pure collaborative filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd376a6f",
   "metadata": {},
   "source": [
    "## 4. Analysis of Interaction Data (`df_big`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f48239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_big.empty:\n",
    "    print(\"\\n--- Interaction Data Analysis (df_big) ---\")\n",
    "    \n",
    "    # Distribution of watch_ratio\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.histplot(df_big['watch_ratio'], bins=50, kde=False)\n",
    "    plt.title('Distribution of Watch Ratio (Big Matrix)')\n",
    "    plt.xlabel('Watch Ratio')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xlim(0, 5) # Zoom in on a practical range\n",
    "    plt.show()\n",
    "    print(df_big['watch_ratio'].describe())\n",
    "    print(f\"Percentage of interactions with watch_ratio > 1: {(df_big['watch_ratio'] > 1).mean()*100:.2f}%\")\n",
    "    print(f\"Percentage of interactions with watch_ratio > 2: {(df_big['watch_ratio'] > 2).mean()*100:.2f}%\")\n",
    "\n",
    "    # Distribution of play_duration and video_duration (log scale due to skew)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "    sns.histplot(np.log1p(df_big['play_duration']), bins=50, ax=axes[0], kde=True)\n",
    "    axes[0].set_title('Distribution of log(Play Duration)')\n",
    "    sns.histplot(np.log1p(df_big['video_duration']), bins=50, ax=axes[1], kde=True)\n",
    "    axes[1].set_title('Distribution of log(Video Duration)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Interactions per user\n",
    "    user_interaction_counts = df_big['user_id'].value_counts()\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.histplot(user_interaction_counts, bins=50, log_scale=(False, True))\n",
    "    plt.title('Distribution of Interactions per User (Big Matrix)')\n",
    "    plt.xlabel('Number of Interactions')\n",
    "    plt.ylabel('Number of Users (log scale)')\n",
    "    plt.show()\n",
    "    print(\"\\nInteractions per user describe:\")\n",
    "    print(user_interaction_counts.describe())\n",
    "\n",
    "    # Interactions per item (video)\n",
    "    item_interaction_counts = df_big['video_id'].value_counts()\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.histplot(item_interaction_counts, bins=50, log_scale=(True, True))\n",
    "    plt.title('Distribution of Interactions per Video (Big Matrix)')\n",
    "    plt.xlabel('Number of Interactions (log scale)')\n",
    "    plt.ylabel('Number of Videos (log scale)')\n",
    "    plt.show()\n",
    "    print(\"\\nInteractions per video describe:\")\n",
    "    print(item_interaction_counts.describe())\n",
    "\n",
    "    # Temporal patterns (if 'time' or 'date' is reasonably formatted)\n",
    "    df_big['interaction_datetime'] = pd.to_datetime(df_big['time'], errors='coerce')\n",
    "    df_big.dropna(subset=['interaction_datetime'], inplace=True) # Drop rows where time couldn't be parsed\n",
    "    \n",
    "    if not df_big.empty:\n",
    "        df_big['interaction_hour'] = df_big['interaction_datetime'].dt.hour\n",
    "        plt.figure(figsize=(10,5))\n",
    "        sns.countplot(data=df_big, x='interaction_hour')\n",
    "        plt.title('Interactions by Hour of Day (Big Matrix)')\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"df_big is empty or became empty after NA drop, skipping interaction analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc11f27",
   "metadata": {},
   "source": [
    "**Justification for Feature Engineering (Interactions):**\n",
    "- The `watch_ratio` distribution shows many values around 0-2, with a long tail. Values > 1 indicate rewatches or watching more than the video length (perhaps due to player UI or looping). This confirms `watch_ratio` is a good continuous target. Using MAE (L1 loss) for LightGBM regression seems appropriate to be robust to outliers.\n",
    "- `play_duration` and `video_duration` are skewed, typical for time-based measures. Log transformation helps in visualization and potentially for some models (though tree models handle raw scale well).\n",
    "- User and item interaction counts show a long-tail distribution (power law), typical in recommender systems. This implies some users are very active, and some items are very popular, while many are not. This reinforces the need for models that can handle this, and features like user/item activity could be useful.\n",
    "- Extracting temporal features like `interaction_hour` and `day_of_week` is justified by observed daily patterns in user activity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198287d6",
   "metadata": {},
   "source": [
    "## 5. Analysis of User Features (`df_users`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e49b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_users.empty:\n",
    "    print(\"\\n--- User Feature Analysis ---\")\n",
    "    print(df_users.head())\n",
    "    print(df_users.info())\n",
    "    print(df_users.describe(include='all'))\n",
    "    \n",
    "    # Categorical user features\n",
    "    if 'user_active_degree' in df_users.columns:\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        sns.countplot(data=df_users, y='user_active_degree', order=df_users['user_active_degree'].value_counts().index)\n",
    "        plt.title('Distribution of User Active Degree')\n",
    "        plt.show()\n",
    "    \n",
    "    # Numerical user features\n",
    "    numerical_user_cols = ['fans_user_num', 'register_days', 'follow_user_num', 'friend_user_num']\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    for i, col in enumerate(numerical_user_cols):\n",
    "        if col in df_users.columns:\n",
    "            sns.histplot(np.log1p(df_users[col]), bins=30, ax=axes[i], kde=True)\n",
    "            axes[i].set_title(f'Distribution of log({col})')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"df_users is empty, skipping user feature analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efb28d0",
   "metadata": {},
   "source": [
    "**Justification for Feature Engineering (User Features):**\n",
    "- Categorical features like `user_active_degree` can be directly used or one-hot encoded. Their distribution shows if certain activity levels are dominant.\n",
    "- Numerical features like `fans_user_num` and `register_days` are skewed, suggesting log transformation might be beneficial for linear models or distance-based algorithms if they were used (though tree models are less sensitive). Keeping them as is for LightGBM is fine. These features directly capture user engagement and tenure, which are likely predictive.\n",
    "- Binary flags like `is_live_streamer`, `is_video_author` directly segment users and are easy to incorporate.\n",
    "- The `onehot_feat*` (not plotted here for brevity) are pre-encoded categorical features and should be included in models that can handle categorical inputs well (like LightGBM)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8be407a",
   "metadata": {},
   "source": [
    "## 6. Analysis of Item Features (`df_items_cat`, `df_items_daily`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595c1267",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_items_cat.empty:\n",
    "    print(\"\\n--- Item Category Analysis ---\")\n",
    "    print(df_items_cat.head())\n",
    "    if 'num_tags' in df_items_cat.columns:\n",
    "        plt.figure(figsize=(8,4))\n",
    "        sns.histplot(df_items_cat['num_tags'], bins=max(1, df_items_cat['num_tags'].max()), discrete=True)\n",
    "        plt.title('Distribution of Number of Tags per Item')\n",
    "        plt.xlabel('Number of Tags')\n",
    "        plt.show()\n",
    "        print(df_items_cat['num_tags'].describe())\n",
    "\n",
    "if not df_items_daily.empty:\n",
    "    print(\"\\n--- Item Daily Features Analysis (Sample) ---\")\n",
    "    print(df_items_daily.head())\n",
    "    print(df_items_daily.info())\n",
    "    \n",
    "    # Distribution of play_progress for a sample of videos\n",
    "    if 'play_progress' in df_items_daily.columns:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        sns.histplot(df_items_daily['play_progress'].dropna(), bins=50, kde=True)\n",
    "        plt.title('Distribution of Daily Play Progress (All Daily Entries)')\n",
    "        plt.xlim(0, df_items_daily['play_progress'].quantile(0.99)) # Zoom on main distribution\n",
    "        plt.show()\n",
    "        print(df_items_daily['play_progress'].describe())\n",
    "    \n",
    "    # Average play_cnt per video over its observed days\n",
    "    if 'play_cnt' in df_items_daily.columns:\n",
    "        avg_daily_play_cnt = df_items_daily.groupby('video_id')['play_cnt'].mean()\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        sns.histplot(np.log1p(avg_daily_play_cnt), bins=50, kde=True)\n",
    "        plt.title('Distribution of log(Average Daily Play Count per Video)')\n",
    "        plt.show()\n",
    "        print(\"\\nAverage Daily Play Count per Video describe:\")\n",
    "        print(avg_daily_play_cnt.describe())\n",
    "    \n",
    "    if 'video_type' in df_items_daily.columns:\n",
    "        plt.figure(figsize=(8,4))\n",
    "        sns.countplot(data=df_items_daily, y='video_type', order=df_items_daily['video_type'].value_counts().index)\n",
    "        plt.title('Distribution of Video Types (Daily Entries)')\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"Item feature dataframes are empty, skipping item feature analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e67c38",
   "metadata": {},
   "source": [
    "**Justification for Feature Engineering (Item Features):**\n",
    "- `num_item_tags` (derived from `item_categories`) provides a simple item content complexity measure.\n",
    "- `item_daily_features` are rich. \n",
    "    - Features like `play_progress` directly measure item engagement on a given day.\n",
    "    - Aggregating daily stats (e.g., average daily play count) can give a general popularity signal for an item. This EDA shows these stats are also skewed.\n",
    "    - Calculating ratios from daily counts (e.g., play/show, like/play) is crucial for normalizing popularity and capturing efficiency, as done in the main data prep notebook. This EDA shows the raw counts are available for such derivations.\n",
    "    - `video_type` is a useful categorical feature.\n",
    "    - `upload_dt` can be used to calculate `video_age_days`, which is a very common and useful feature representing item freshness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fdb247",
   "metadata": {},
   "source": [
    "## 7. Correlating Features with Target (Watch Ratio)\n",
    "\n",
    "To get a preliminary idea of feature importance, we can compute average `watch_ratio` grouped by some key categorical features, or look at correlations for numerical features. This requires joining feature tables with interaction data aggregates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ddd6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_big.empty and not df_users.empty:\n",
    "    print(\"\\n--- Correlating User Features with Watch Ratio (using df_big) ---\")\n",
    "    # Calculate average watch_ratio per user\n",
    "    avg_watch_ratio_per_user = df_big.groupby('user_id')['watch_ratio'].mean().reset_index()\n",
    "    avg_watch_ratio_per_user.rename(columns={'watch_ratio': 'avg_user_watch_ratio'}, inplace=True)\n",
    "    \n",
    "    # Merge with user features\n",
    "    df_users_eda_merged = pd.merge(df_users, avg_watch_ratio_per_user, on='user_id', how='left')\n",
    "    \n",
    "    if 'user_active_degree' in df_users_eda_merged.columns:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(data=df_users_eda_merged, x='user_active_degree', y='avg_user_watch_ratio',\n",
    "                    order=df_users_eda_merged.groupby('user_active_degree')['avg_user_watch_ratio'].median().sort_values(ascending=False).index)\n",
    "        plt.title('Average Watch Ratio by User Active Degree')\n",
    "        plt.ylim(0, df_users_eda_merged['avg_user_watch_ratio'].quantile(0.95)) # Zoom\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()\n",
    "        \n",
    "    numerical_user_cols_for_corr = ['fans_user_num', 'register_days', 'follow_user_num']\n",
    "    for col in numerical_user_cols_for_corr:\n",
    "        if col in df_users_eda_merged.columns and 'avg_user_watch_ratio' in df_users_eda_merged.columns:\n",
    "            # Calculate correlation on log-transformed data if skewed, or rank correlation\n",
    "            if col == 'fans_user_num':\n",
    "                plt.figure(figsize=(8,5))\n",
    "                # Sample to avoid overplotting if df is large\n",
    "                sample_df = df_users_eda_merged.sample(min(len(df_users_eda_merged), 5000))\n",
    "                sns.scatterplot(data=sample_df, x=np.log1p(sample_df[col]), y=sample_df['avg_user_watch_ratio'])\n",
    "                plt.title(f'log({col}) vs. Avg User Watch Ratio')\n",
    "                plt.ylim(0, sample_df['avg_user_watch_ratio'].quantile(0.98))\n",
    "                plt.show()\n",
    "            correlation = df_users_eda_merged[col].corr(df_users_eda_merged['avg_user_watch_ratio'], method='spearman')\n",
    "            print(f\"Spearman correlation between {col} and avg_user_watch_ratio: {correlation:.2f}\")\n",
    "else:\n",
    "    print(\"Skipping user feature correlation with watch ratio due to missing dataframes.\")\n",
    "\n",
    "if not df_big.empty and not df_items_daily.empty:\n",
    "    print(\"\\n--- Correlating Item Daily Features with Watch Ratio (using df_big) ---\")\n",
    "    # Calculate average watch_ratio per item\n",
    "    avg_watch_ratio_per_item = df_big.groupby('video_id')['watch_ratio'].mean().reset_index()\n",
    "    avg_watch_ratio_per_item.rename(columns={'watch_ratio': 'avg_item_watch_ratio'}, inplace=True)\n",
    "    \n",
    "    # We need an average of daily stats per video_id from df_items_daily\n",
    "    item_daily_summary_eda = df_items_daily.groupby('video_id').agg(\n",
    "        avg_daily_play_progress=('play_progress', 'mean'),\n",
    "        video_type_mode=('video_type', lambda x: x.mode()[0] if not x.mode().empty else 'Unknown')\n",
    "    ).reset_index()\n",
    "    \n",
    "    df_items_eda_merged = pd.merge(item_daily_summary_eda, avg_watch_ratio_per_item, on='video_id', how='inner')\n",
    "    \n",
    "    if 'avg_daily_play_progress' in df_items_eda_merged.columns:\n",
    "        correlation = df_items_eda_merged['avg_daily_play_progress'].corr(df_items_eda_merged['avg_item_watch_ratio'])\n",
    "        print(f\"Pearson correlation between avg_daily_play_progress and avg_item_watch_ratio: {correlation:.2f}\")\n",
    "        plt.figure(figsize=(8,5))\n",
    "        sample_df = df_items_eda_merged.sample(min(len(df_items_eda_merged), 5000))\n",
    "        sns.scatterplot(data=sample_df, x='avg_daily_play_progress', y='avg_item_watch_ratio')\n",
    "        plt.title('Avg Daily Play Progress vs. Avg Item Watch Ratio')\n",
    "        plt.ylim(0, sample_df['avg_item_watch_ratio'].quantile(0.98))\n",
    "        plt.xlim(0, sample_df['avg_daily_play_progress'].quantile(0.98))\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"Skipping item feature correlation with watch ratio due to missing dataframes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a6283f",
   "metadata": {},
   "source": [
    "**Justification for Feature Importance (Preliminary):**\n",
    "- Features showing clear trends or correlations with average `watch_ratio` (even if aggregated for EDA) are good candidates for inclusion in the feature set. For example, `user_active_degree` seems to influence average watch ratios. `fans_user_num` and `avg_daily_play_progress` also show some correlation.\n",
    "- This justifies keeping these types of features in the main data preparation pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9f6884",
   "metadata": {},
   "source": [
    "## 8. EDA Summary & Next Steps\n",
    "\n",
    "This initial EDA has provided several key insights:\n",
    "1.  **Data Characteristics:** We have a large, sparse `big_matrix` for training and a smaller, dense `small_matrix` which will be valuable for a specific type of evaluation.\n",
    "2.  **Target Variable:** `watch_ratio` is a continuous variable, skewed, with values often exceeding 1.0. This supports regression models and loss functions robust to outliers (like L1/MAE).\n",
    "3.  **Feature Relevance:** User activity, tenure, popularity (fans), item daily engagement metrics (like play progress), and item categories (number of tags) show potential relationships with the target and user/item activity. Temporal features (hour, day of week, video age) are also important.\n",
    "4.  **Data Quality:** The `item_daily_features` file contained duplicate entries for `(video_id, date)`, which needed to be handled in the main data preparation pipeline to prevent issues during merges.\n",
    "\n",
    "**Implications for Modeling & Feature Engineering:**\n",
    "- The choice of a two-stage model (ALS for candidates, LightGBM for ranking) is well-supported. ALS handles sparsity for candidate generation, and LightGBM can leverage the rich feature set for fine-grained ranking.\n",
    "- The feature engineering steps in the main data preparation notebook (e.g., creating ratios from daily stats, calculating video age, extracting temporal features) are justified by the patterns observed here.\n",
    "- The selected features (`user_active_degree`, `fans_user_num`, `register_days`, `onehot_feat*` for users; `num_item_tags`, daily ratios, `video_age_days`, `author_id`, `video_type` for items) seem like a good starting point.\n",
    "\n",
    "This EDA provides a foundation for the choices made in subsequent data preparation, model training, and evaluation notebooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
